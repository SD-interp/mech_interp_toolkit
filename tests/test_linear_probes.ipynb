{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test: linear_probes module\n",
    "\n",
    "This notebook tests the `LinearProbe` class from `mech_interp_toolkit.linear_probes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoConfig\n",
    "from mech_interp_toolkit.linear_probes import LinearProbe\n",
    "from mech_interp_toolkit.activation_dict import ActivationDict\n",
    "from mech_interp_toolkit.utils import set_global_seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "set_global_seed(42)\n",
    "\n",
    "# Load config\n",
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "print(f\"Loaded config for {model_name}\")\n",
    "print(f\"Hidden size: {config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: LinearProbe initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test classification initialization\n",
    "probe_cls = LinearProbe(target_type=\"classification\")\n",
    "print(f\"Classification probe model: {type(probe_cls.linear_model).__name__}\")\n",
    "assert probe_cls.target_type == \"classification\"\n",
    "assert probe_cls.broadcast_target == True\n",
    "assert probe_cls.test_split == 0.2\n",
    "print(\"PASSED: Classification initialization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test regression initialization\n",
    "probe_reg = LinearProbe(target_type=\"regression\", test_split=0.3)\n",
    "print(f\"Regression probe model: {type(probe_reg.linear_model).__name__}\")\n",
    "assert probe_reg.target_type == \"regression\"\n",
    "assert probe_reg.test_split == 0.3\n",
    "print(\"PASSED: Regression initialization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test invalid target_type\n",
    "try:\n",
    "    probe = LinearProbe(target_type=\"invalid\")\n",
    "    assert False, \"Should have raised ValueError\"\n",
    "except ValueError as e:\n",
    "    print(f\"Correctly raised error: {e}\")\n",
    "print(\"PASSED: Invalid target_type error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test invalid test_split\n",
    "try:\n",
    "    probe = LinearProbe(target_type=\"classification\", test_split=0.0)\n",
    "    assert False, \"Should have raised ValueError\"\n",
    "except ValueError as e:\n",
    "    print(f\"Correctly raised error for test_split=0.0: {e}\")\n",
    "\n",
    "try:\n",
    "    probe = LinearProbe(target_type=\"classification\", test_split=1.0)\n",
    "    assert False, \"Should have raised ValueError\"\n",
    "except ValueError as e:\n",
    "    print(f\"Correctly raised error for test_split=1.0: {e}\")\n",
    "\n",
    "print(\"PASSED: Invalid test_split error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Classification probe (2D inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_global_seed(42)\n",
    "\n",
    "# Create synthetic classification data\n",
    "n_samples = 200\n",
    "d_model = config.hidden_size\n",
    "\n",
    "# Create 2D inputs (batch, d_model) - single position\n",
    "X_data = torch.randn(n_samples, d_model)\n",
    "# Create linearly separable targets\n",
    "true_weights = torch.randn(d_model)\n",
    "y_data = (X_data @ true_weights > 0).long().numpy()\n",
    "\n",
    "# Wrap in ActivationDict\n",
    "X_act = ActivationDict(config, positions=slice(None))\n",
    "X_act[(0, \"mlp\")] = X_data\n",
    "\n",
    "print(f\"X shape: {X_data.shape}\")\n",
    "print(f\"y shape: {y_data.shape}\")\n",
    "print(f\"y distribution: {np.bincount(y_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classification probe\n",
    "probe = LinearProbe(target_type=\"classification\", test_split=0.2)\n",
    "probe.fit(X_act, y_data)\n",
    "\n",
    "print(f\"\\nWeight shape: {probe.weight.shape}\")\n",
    "print(f\"Bias shape: {probe.bias.shape}\")\n",
    "\n",
    "assert probe.weight is not None, \"Weight should be set after fitting\"\n",
    "assert probe.bias is not None, \"Bias should be set after fitting\"\n",
    "print(\"PASSED: Classification probe training (2D)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Classification probe (3D inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_global_seed(42)\n",
    "\n",
    "# Create 3D inputs (batch, positions, d_model)\n",
    "n_samples = 100\n",
    "n_positions = 5\n",
    "d_model = config.hidden_size\n",
    "\n",
    "X_data_3d = torch.randn(n_samples, n_positions, d_model)\n",
    "# Single target per sample (broadcast across positions)\n",
    "y_data = np.random.randint(0, 2, n_samples)\n",
    "\n",
    "X_act_3d = ActivationDict(config, positions=slice(None))\n",
    "X_act_3d[(0, \"mlp\")] = X_data_3d\n",
    "\n",
    "print(f\"X 3D shape: {X_data_3d.shape}\")\n",
    "print(f\"y shape: {y_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with broadcast_target=True (default)\n",
    "probe_broadcast = LinearProbe(target_type=\"classification\", broadcast_target=True)\n",
    "probe_broadcast.fit(X_act_3d, y_data)\n",
    "\n",
    "print(f\"Weight shape: {probe_broadcast.weight.shape}\")\n",
    "print(\"PASSED: Classification probe with broadcast_target=True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with broadcast_target=False (token-level targets)\n",
    "# Need targets for each position\n",
    "y_data_tokens = np.random.randint(0, 2, (n_samples, n_positions))\n",
    "\n",
    "probe_no_broadcast = LinearProbe(target_type=\"classification\", broadcast_target=False)\n",
    "probe_no_broadcast.fit(X_act_3d, y_data_tokens)\n",
    "\n",
    "print(f\"Weight shape: {probe_no_broadcast.weight.shape}\")\n",
    "print(\"PASSED: Classification probe with broadcast_target=False\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Regression probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_global_seed(42)\n",
    "\n",
    "# Create regression data\n",
    "n_samples = 200\n",
    "d_model = config.hidden_size\n",
    "\n",
    "X_data = torch.randn(n_samples, d_model)\n",
    "true_weights = torch.randn(d_model)\n",
    "y_data = (X_data @ true_weights + 0.1 * torch.randn(n_samples)).numpy()\n",
    "\n",
    "X_act = ActivationDict(config, positions=slice(None))\n",
    "X_act[(0, \"mlp\")] = X_data\n",
    "\n",
    "print(f\"X shape: {X_data.shape}\")\n",
    "print(f\"y shape: {y_data.shape}\")\n",
    "print(f\"y range: [{y_data.min():.2f}, {y_data.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train regression probe\n",
    "probe_reg = LinearProbe(target_type=\"regression\", test_split=0.2)\n",
    "probe_reg.fit(X_act, y_data)\n",
    "\n",
    "print(f\"\\nWeight shape: {probe_reg.weight.shape}\")\n",
    "print(f\"Bias: {probe_reg.bias}\")\n",
    "print(\"PASSED: Regression probe training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_global_seed(42)\n",
    "\n",
    "# Create and train a classification probe\n",
    "n_train = 200\n",
    "n_test = 50\n",
    "\n",
    "X_train = torch.randn(n_train, config.hidden_size)\n",
    "y_train = np.random.randint(0, 2, n_train)\n",
    "\n",
    "X_train_act = ActivationDict(config, positions=slice(None))\n",
    "X_train_act[(0, \"mlp\")] = X_train\n",
    "\n",
    "probe = LinearProbe(target_type=\"classification\")\n",
    "probe.fit(X_train_act, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on new data\n",
    "X_test = torch.randn(n_test, config.hidden_size)\n",
    "y_test = np.random.randint(0, 2, n_test)\n",
    "\n",
    "X_test_act = ActivationDict(config, positions=slice(None))\n",
    "X_test_act[(0, \"mlp\")] = X_test\n",
    "\n",
    "# Predict without targets\n",
    "print(\"Prediction without targets:\")\n",
    "preds = probe.predict(X_test_act)\n",
    "print(f\"Predictions shape: {preds.shape}\")\n",
    "print(f\"Unique predictions: {np.unique(preds)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict with targets (shows metrics)\n",
    "print(\"\\nPrediction with targets:\")\n",
    "preds_with_metrics = probe.predict(X_test_act, target=y_test, label=\"Test\")\n",
    "print(f\"Predictions shape: {preds_with_metrics.shape}\")\n",
    "print(\"PASSED: Prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predict on unfitted probe\n",
    "unfitted_probe = LinearProbe(target_type=\"classification\")\n",
    "\n",
    "try:\n",
    "    unfitted_probe.predict(X_test_act)\n",
    "    assert False, \"Should have raised ValueError\"\n",
    "except ValueError as e:\n",
    "    print(f\"Correctly raised error: {e}\")\n",
    "\n",
    "print(\"PASSED: Unfitted probe error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Error handling for multiple components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LinearProbe only supports single component in ActivationDict\n",
    "X_multi = ActivationDict(config, positions=slice(None))\n",
    "X_multi[(0, \"attn\")] = torch.randn(100, config.hidden_size)\n",
    "X_multi[(0, \"mlp\")] = torch.randn(100, config.hidden_size)\n",
    "\n",
    "y = np.random.randint(0, 2, 100)\n",
    "\n",
    "probe = LinearProbe(target_type=\"classification\")\n",
    "\n",
    "try:\n",
    "    probe.fit(X_multi, y)\n",
    "    assert False, \"Should have raised ValueError\"\n",
    "except ValueError as e:\n",
    "    print(f\"Correctly raised error: {e}\")\n",
    "\n",
    "print(\"PASSED: Multiple components error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Using tensors as targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_global_seed(42)\n",
    "\n",
    "X = torch.randn(100, config.hidden_size)\n",
    "y_tensor = torch.randint(0, 2, (100,))\n",
    "\n",
    "X_act = ActivationDict(config, positions=slice(None))\n",
    "X_act[(0, \"mlp\")] = X\n",
    "\n",
    "probe = LinearProbe(target_type=\"classification\")\n",
    "probe.fit(X_act, y_tensor)  # Pass tensor instead of numpy array\n",
    "\n",
    "print(f\"Weight shape: {probe.weight.shape}\")\n",
    "print(\"PASSED: Tensor targets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: With real activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mech_interp_toolkit.utils import load_model_tokenizer_config, get_default_device\n",
    "from mech_interp_toolkit.activations import UnifiedAccessAndPatching\n",
    "\n",
    "# Load model\n",
    "device = get_default_device()\n",
    "model, tok, cfg = load_model_tokenizer_config(model_name, device=device)\n",
    "print(f\"Model loaded on {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate activations for \"positive\" and \"negative\" prompts\n",
    "positive_prompts = [\n",
    "    \"This is great!\",\n",
    "    \"I love this!\",\n",
    "    \"Amazing work!\",\n",
    "    \"Wonderful!\",\n",
    "    \"Excellent job!\",\n",
    "] * 10  # Repeat for more samples\n",
    "\n",
    "negative_prompts = [\n",
    "    \"This is terrible!\",\n",
    "    \"I hate this!\",\n",
    "    \"Awful work!\",\n",
    "    \"Horrible!\",\n",
    "    \"Bad job!\",\n",
    "] * 10\n",
    "\n",
    "all_prompts = positive_prompts + negative_prompts\n",
    "labels = np.array([1] * len(positive_prompts) + [0] * len(negative_prompts))\n",
    "\n",
    "print(f\"Total samples: {len(all_prompts)}\")\n",
    "print(f\"Labels: {np.bincount(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract activations\n",
    "inputs = tok(all_prompts, thinking=False)\n",
    "\n",
    "spec_dict = {\n",
    "    \"activations\": {\n",
    "        \"positions\": -1,  # Last position\n",
    "        \"locations\": [(10, \"mlp\")],  # Middle layer\n",
    "    }\n",
    "}\n",
    "\n",
    "with UnifiedAccessAndPatching(model, inputs, spec_dict) as uap:\n",
    "    activations, _ = uap.unified_access_and_patching()\n",
    "\n",
    "print(f\"Activations shape: {activations[(10, 'mlp')].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train linear probe on real activations\n",
    "probe = LinearProbe(target_type=\"classification\", test_split=0.2)\n",
    "probe.fit(activations, labels)\n",
    "\n",
    "print(f\"\\nWeight shape: {probe.weight.shape}\")\n",
    "print(\"PASSED: Training on real activations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"All linear_probes module tests PASSED!\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
