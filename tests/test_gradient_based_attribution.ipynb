{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test: gradient_based_attribution module\n",
    "\n",
    "This notebook tests gradient-based attribution methods from `mech_interp_toolkit.gradient_based_attribution`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "import torch\n",
    "from mech_interp_toolkit.utils import load_model_tokenizer_config, get_default_device, get_all_layer_components\n",
    "from mech_interp_toolkit.activation_dict import ActivationDict\n",
    "from mech_interp_toolkit.gradient_based_attribution import (\n",
    "    edge_attribution_patching,\n",
    "    simple_integrated_gradients,\n",
    "    eap_integrated_gradients,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "device = get_default_device()\n",
    "\n",
    "print(f\"Loading model {model_name} on {device}...\")\n",
    "model, tokenizer, config = load_model_tokenizer_config(model_name, device=device)\n",
    "print(\"Model loaded successfully\")\n",
    "print(f\"Number of layers: {config.num_hidden_layers}\")\n",
    "print(f\"Hidden size: {config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare clean and corrupted inputs for EAP\n",
    "clean_prompts = [\"The capital of France is\"]\n",
    "corrupted_prompts = [\"The capital of Germany is\"]\n",
    "\n",
    "clean_inputs = tokenizer(clean_prompts, thinking=False)\n",
    "corrupted_inputs = tokenizer(corrupted_prompts, thinking=False)\n",
    "\n",
    "print(f\"Clean input shape: {clean_inputs['input_ids'].shape}\")\n",
    "print(f\"Corrupted input shape: {corrupted_inputs['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: edge_attribution_patching()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple metric function\n",
    "def metric_fn(logits):\n",
    "    return logits[:, -1, :].max(dim=-1).values.sum()\n",
    "\n",
    "print(\"Running Edge Attribution Patching (EAP)...\")\n",
    "eap_scores = edge_attribution_patching(\n",
    "    model=model,\n",
    "    clean_inputs=clean_inputs,\n",
    "    corrupted_inputs=corrupted_inputs,\n",
    "    compute_grad_at=\"clean\",\n",
    "    metric_fn=metric_fn,\n",
    "    position=-1,\n",
    ")\n",
    "\n",
    "print(f\"\\nEAP scores computed for {len(eap_scores)} components\")\n",
    "print(f\"EAP score type: {type(eap_scores)}\")\n",
    "print(f\"\\nSample EAP scores:\")\n",
    "for key, score in list(eap_scores.items())[:6]:\n",
    "    print(f\"  {key}: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze EAP scores\n",
    "n_layers = config.num_hidden_layers\n",
    "\n",
    "print(\"\\nEAP scores summary by layer:\")\n",
    "print(f\"{'Layer':<8} {'Attn':>12} {'MLP':>12}\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "for layer in range(n_layers):\n",
    "    attn_key = (layer, \"attn\")\n",
    "    mlp_key = (layer, \"mlp\")\n",
    "    \n",
    "    attn_score = eap_scores.get(attn_key, torch.tensor(0.0)).item() if attn_key in eap_scores else 0.0\n",
    "    mlp_score = eap_scores.get(mlp_key, torch.tensor(0.0)).item() if mlp_key in eap_scores else 0.0\n",
    "    \n",
    "    print(f\"{layer:<8} {attn_score:>12.6f} {mlp_score:>12.6f}\")\n",
    "\n",
    "print(\"PASSED: Edge Attribution Patching\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with compute_grad_at=\"corrupted\"\n",
    "print(\"\\nRunning EAP with compute_grad_at='corrupted'...\")\n",
    "eap_scores_corrupted = edge_attribution_patching(\n",
    "    model=model,\n",
    "    clean_inputs=clean_inputs,\n",
    "    corrupted_inputs=corrupted_inputs,\n",
    "    compute_grad_at=\"corrupted\",\n",
    "    metric_fn=metric_fn,\n",
    "    position=-1,\n",
    ")\n",
    "\n",
    "print(f\"EAP scores (corrupted): {len(eap_scores_corrupted)} components\")\n",
    "print(\"Sample scores:\")\n",
    "for key in list(eap_scores_corrupted.keys())[:3]:\n",
    "    print(f\"  {key}: {eap_scores_corrupted[key]}\")\n",
    "\n",
    "print(\"PASSED: EAP with corrupted gradients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: simple_integrated_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline embeddings (zeros)\n",
    "inputs = tokenizer([\"The quick brown fox\"], thinking=False)\n",
    "seq_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "baseline_embeddings = ActivationDict(config, positions=slice(None))\n",
    "baseline_embeddings[(0, \"layer_in\")] = torch.zeros(\n",
    "    1, seq_len, config.hidden_size, \n",
    "    device=device, \n",
    "    dtype=model.dtype\n",
    ")\n",
    "\n",
    "print(f\"Input sequence length: {seq_len}\")\n",
    "print(f\"Baseline embeddings shape: {baseline_embeddings[(0, 'layer_in')].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running Simple Integrated Gradients...\")\n",
    "ig_attributions = simple_integrated_gradients(\n",
    "    model=model,\n",
    "    inputs=inputs,\n",
    "    baseline_embeddings=baseline_embeddings,\n",
    "    metric_fn=metric_fn,\n",
    "    steps=10,  # Using fewer steps for faster testing\n",
    ")\n",
    "\n",
    "print(f\"\\nIG attributions computed\")\n",
    "print(f\"Keys: {list(ig_attributions.keys())}\")\n",
    "\n",
    "for key, val in ig_attributions.items():\n",
    "    print(f\"  {key}: shape={val.shape}\")\n",
    "\n",
    "print(\"PASSED: Simple Integrated Gradients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze IG attributions per position\n",
    "ig_values = ig_attributions[(0, \"layer_in\")]\n",
    "print(f\"\\nIG attribution shape: {ig_values.shape}\")\n",
    "print(f\"\\nIG attribution per position:\")\n",
    "\n",
    "for pos in range(ig_values.shape[1]):\n",
    "    print(f\"  Position {pos}: {ig_values[0, pos].item():.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different step counts\n",
    "print(\"\\nTesting IG with different step counts:\")\n",
    "for steps in [5, 10, 20]:\n",
    "    ig = simple_integrated_gradients(\n",
    "        model=model,\n",
    "        inputs=inputs,\n",
    "        baseline_embeddings=baseline_embeddings,\n",
    "        metric_fn=metric_fn,\n",
    "        steps=steps,\n",
    "    )\n",
    "    total_attribution = ig[(0, \"layer_in\")].sum().item()\n",
    "    print(f\"  Steps={steps}: total attribution = {total_attribution:.6f}\")\n",
    "\n",
    "print(\"PASSED: IG with different step counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: eap_integrated_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get baseline embeddings for EAP-IG\n",
    "inputs = tokenizer([\"The capital of France is\"], thinking=False)\n",
    "seq_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "baseline_embeddings = ActivationDict(config, positions=slice(None))\n",
    "baseline_embeddings[(0, \"layer_in\")] = torch.zeros(\n",
    "    1, seq_len, config.hidden_size,\n",
    "    device=device,\n",
    "    dtype=model.dtype\n",
    ")\n",
    "\n",
    "print(f\"Input shape: {inputs['input_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run EAP-IG with specific layer components\n",
    "layer_components = [(0, \"attn\"), (0, \"mlp\"), (5, \"attn\"), (5, \"mlp\")]\n",
    "\n",
    "print(\"Running EAP Integrated Gradients...\")\n",
    "eap_ig = eap_integrated_gradients(\n",
    "    model=model,\n",
    "    inputs=inputs,\n",
    "    baseline_embeddings=baseline_embeddings,\n",
    "    layer_components=layer_components,\n",
    "    metric_fn=metric_fn,\n",
    "    position=-1,\n",
    "    intermediate_points=5,\n",
    ")\n",
    "\n",
    "print(f\"\\nEAP-IG computed for {len(eap_ig)} components\")\n",
    "for key, val in eap_ig.items():\n",
    "    print(f\"  {key}: {val}\")\n",
    "\n",
    "print(\"PASSED: EAP Integrated Gradients\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run EAP-IG with all layer components (default)\n",
    "print(\"\\nRunning EAP-IG with all layer components...\")\n",
    "eap_ig_full = eap_integrated_gradients(\n",
    "    model=model,\n",
    "    inputs=inputs,\n",
    "    baseline_embeddings=baseline_embeddings,\n",
    "    layer_components=None,  # Uses all components\n",
    "    metric_fn=metric_fn,\n",
    "    position=-1,\n",
    "    intermediate_points=3,\n",
    ")\n",
    "\n",
    "print(f\"EAP-IG (full): {len(eap_ig_full)} components\")\n",
    "\n",
    "# Print summary by layer\n",
    "print(\"\\nEAP-IG summary by layer:\")\n",
    "print(f\"{'Layer':<8} {'Attn':>12} {'MLP':>12}\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "for layer in range(min(10, config.num_hidden_layers)):\n",
    "    attn_val = eap_ig_full.get((layer, \"attn\"), torch.tensor(0.0))\n",
    "    mlp_val = eap_ig_full.get((layer, \"mlp\"), torch.tensor(0.0))\n",
    "    attn_score = attn_val.item() if isinstance(attn_val, torch.Tensor) else attn_val\n",
    "    mlp_score = mlp_val.item() if isinstance(mlp_val, torch.Tensor) else mlp_val\n",
    "    print(f\"{layer:<8} {attn_score:>12.6f} {mlp_score:>12.6f}\")\n",
    "\n",
    "print(\"PASSED: EAP-IG with all components\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Custom metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different metric functions\n",
    "def metric_sum(logits):\n",
    "    return logits.sum()\n",
    "\n",
    "def metric_mean(logits):\n",
    "    return logits.mean()\n",
    "\n",
    "def metric_max_prob(logits):\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    return probs.max()\n",
    "\n",
    "metrics = [\n",
    "    (\"sum\", metric_sum),\n",
    "    (\"mean\", metric_mean),\n",
    "    (\"max_prob\", metric_max_prob),\n",
    "]\n",
    "\n",
    "print(\"Testing EAP with different metrics:\")\n",
    "for name, metric in metrics:\n",
    "    eap = edge_attribution_patching(\n",
    "        model=model,\n",
    "        clean_inputs=clean_inputs,\n",
    "        corrupted_inputs=corrupted_inputs,\n",
    "        metric_fn=metric,\n",
    "        position=-1,\n",
    "    )\n",
    "    # Get total attribution\n",
    "    total = sum(v.sum().item() for v in eap.values())\n",
    "    print(f\"  {name}: total attribution = {total:.6f}\")\n",
    "\n",
    "print(\"PASSED: Custom metric functions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Different positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test EAP with different position specifications\n",
    "print(\"Testing EAP with different positions:\")\n",
    "\n",
    "# Last position\n",
    "eap_last = edge_attribution_patching(\n",
    "    model=model,\n",
    "    clean_inputs=clean_inputs,\n",
    "    corrupted_inputs=corrupted_inputs,\n",
    "    metric_fn=metric_fn,\n",
    "    position=-1,\n",
    ")\n",
    "print(f\"  position=-1: {len(eap_last)} components\")\n",
    "\n",
    "# All positions\n",
    "eap_all = edge_attribution_patching(\n",
    "    model=model,\n",
    "    clean_inputs=clean_inputs,\n",
    "    corrupted_inputs=corrupted_inputs,\n",
    "    metric_fn=metric_fn,\n",
    "    position=slice(None),\n",
    ")\n",
    "print(f\"  position=slice(None): {len(eap_all)} components\")\n",
    "\n",
    "print(\"PASSED: Different positions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"All gradient_based_attribution module tests PASSED!\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
