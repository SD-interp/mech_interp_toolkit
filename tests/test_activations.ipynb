{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test: activations module\n",
    "\n",
    "This notebook tests `UnifiedAccessAndPatching` and `create_z_patch_dict` from `mech_interp_toolkit.activations`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "import torch\n",
    "from mech_interp_toolkit.utils import load_model_tokenizer_config, get_default_device\n",
    "from mech_interp_toolkit.activations import UnifiedAccessAndPatching, create_z_patch_dict\n",
    "from mech_interp_toolkit.activation_dict import ActivationDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "device = get_default_device()\n",
    "\n",
    "print(f\"Loading model {model_name} on {device}...\")\n",
    "model, tokenizer, config = load_model_tokenizer_config(model_name, device=device)\n",
    "print(\"Model loaded successfully\")\n",
    "print(f\"Number of layers: {config.num_hidden_layers}\")\n",
    "print(f\"Hidden size: {config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test inputs\n",
    "prompts = [\n",
    "    \"The capital of France is\",\n",
    "    \"The capital of Germany is\"\n",
    "]\n",
    "inputs = tokenizer(prompts, thinking=False)\n",
    "print(f\"Input IDs shape: {inputs['input_ids'].shape}\")\n",
    "print(f\"Attention mask shape: {inputs['attention_mask'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Basic activation extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = config.num_hidden_layers\n",
    "\n",
    "# Extract activations from multiple components\n",
    "spec_dict = {\n",
    "    \"activations\": {\n",
    "        \"positions\": -1,  # Last position only\n",
    "        \"locations\": [\n",
    "            (0, \"layer_in\"),\n",
    "            (0, \"attn\"),\n",
    "            (0, \"mlp\"),\n",
    "            (n_layers - 1, \"layer_out\")\n",
    "        ],\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Extracting activations...\")\n",
    "with UnifiedAccessAndPatching(model, inputs, spec_dict) as uap:\n",
    "    activations, logits = uap.unified_access_and_patching()\n",
    "\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"\\nExtracted activations:\")\n",
    "for key, val in activations.items():\n",
    "    print(f\"  {key}: {val.shape}\")\n",
    "\n",
    "assert (0, \"layer_in\") in activations, \"Should have layer_in\"\n",
    "assert (0, \"attn\") in activations, \"Should have attn\"\n",
    "assert (0, \"mlp\") in activations, \"Should have mlp\"\n",
    "assert (n_layers - 1, \"layer_out\") in activations, \"Should have layer_out\"\n",
    "print(\"PASSED: Basic activation extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Extract multiple positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all positions\n",
    "spec_dict = {\n",
    "    \"activations\": {\n",
    "        \"positions\": slice(None),  # All positions\n",
    "        \"locations\": [(0, \"layer_in\")],\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Extracting all positions...\")\n",
    "with UnifiedAccessAndPatching(model, inputs, spec_dict) as uap:\n",
    "    activations, logits = uap.unified_access_and_patching()\n",
    "\n",
    "seq_len = inputs[\"input_ids\"].shape[1]\n",
    "print(f\"Input sequence length: {seq_len}\")\n",
    "print(f\"Extracted activation shape: {activations[(0, 'layer_in')].shape}\")\n",
    "\n",
    "assert activations[(0, \"layer_in\")].shape[1] == seq_len, \"Should have all positions\"\n",
    "print(\"PASSED: Multiple positions extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific positions\n",
    "spec_dict = {\n",
    "    \"activations\": {\n",
    "        \"positions\": [-3, -2, -1],  # Last 3 positions\n",
    "        \"locations\": [(0, \"layer_in\")],\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Extracting last 3 positions...\")\n",
    "with UnifiedAccessAndPatching(model, inputs, spec_dict) as uap:\n",
    "    activations, logits = uap.unified_access_and_patching()\n",
    "\n",
    "print(f\"Extracted activation shape: {activations[(0, 'layer_in')].shape}\")\n",
    "assert activations[(0, \"layer_in\")].shape[1] == 3, \"Should have 3 positions\"\n",
    "print(\"PASSED: Specific positions extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Extract 'z' activations (attention head outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_dict = {\n",
    "    \"activations\": {\n",
    "        \"positions\": -1,\n",
    "        \"locations\": [(0, \"z\"), (5, \"z\")],\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Extracting z activations...\")\n",
    "with UnifiedAccessAndPatching(model, inputs, spec_dict) as uap:\n",
    "    activations, logits = uap.unified_access_and_patching()\n",
    "\n",
    "print(f\"Layer 0 z shape: {activations[(0, 'z')].shape}\")\n",
    "print(f\"Layer 5 z shape: {activations[(5, 'z')].shape}\")\n",
    "\n",
    "# z activations should have shape (batch, pos, hidden_size) when fused\n",
    "assert activations[(0, \"z\")].shape[-1] == config.hidden_size, \"z should have hidden_size dimension\"\n",
    "print(\"PASSED: z activation extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Activation patching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, get clean activations and logits\n",
    "spec_dict_clean = {\n",
    "    \"activations\": {\n",
    "        \"positions\": -1,\n",
    "        \"locations\": [(n_layers - 1, \"layer_out\")],\n",
    "    }\n",
    "}\n",
    "\n",
    "with UnifiedAccessAndPatching(model, inputs, spec_dict_clean) as uap:\n",
    "    clean_acts, clean_logits = uap.unified_access_and_patching()\n",
    "\n",
    "print(f\"Clean logits shape: {clean_logits.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create patch: zero-ablate MLP at layer 5\n",
    "layer_to_patch = 5\n",
    "batch_size = inputs[\"input_ids\"].shape[0]\n",
    "seq_len = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "patch_data = ActivationDict(config, positions=slice(None))\n",
    "patch_tensor = torch.zeros((batch_size, seq_len, config.hidden_size), device=device, dtype=model.dtype)\n",
    "patch_data[(layer_to_patch, \"mlp\")] = patch_tensor\n",
    "\n",
    "patch_spec = {\n",
    "    \"patching\": patch_data,\n",
    "    \"activations\": {\n",
    "        \"positions\": -1,\n",
    "        \"locations\": [(n_layers - 1, \"layer_out\")],\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"Patching layer {layer_to_patch} MLP with zeros...\")\n",
    "with UnifiedAccessAndPatching(model, inputs, patch_spec) as uap:\n",
    "    patched_acts, patched_logits = uap.unified_access_and_patching()\n",
    "\n",
    "print(f\"Patched logits shape: {patched_logits.shape}\")\n",
    "\n",
    "# Logits should be different after patching\n",
    "diff = (clean_logits - patched_logits).abs().sum()\n",
    "print(f\"Logit difference (clean vs patched): {diff.item():.4f}\")\n",
    "assert diff.item() > 0, \"Patched logits should differ from clean\"\n",
    "print(\"PASSED: Activation patching\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Gradient computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple metric function\n",
    "def metric_fn(logits):\n",
    "    return logits.max(dim=-1).values.sum()\n",
    "\n",
    "spec_dict_grad = {\n",
    "    \"activations\": {\n",
    "        \"positions\": slice(None),  # Need all positions for gradients\n",
    "        \"locations\": [(0, \"layer_in\"), (5, \"attn\")],\n",
    "        \"gradients\": {\n",
    "            \"metric_fn\": metric_fn,\n",
    "            \"compute_metric_at\": (-1, \"logits\")\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Computing gradients...\")\n",
    "with UnifiedAccessAndPatching(model, inputs, spec_dict_grad) as uap:\n",
    "    acts_with_grad, logits = uap.unified_access_and_patching()\n",
    "\n",
    "print(f\"\\nActivations with gradient info:\")\n",
    "for key, val in acts_with_grad.items():\n",
    "    print(f\"  {key}: shape={val.shape}, has_grad={val.grad is not None}\")\n",
    "    if val.grad is not None:\n",
    "        print(f\"    grad shape: {val.grad.shape}, grad norm: {val.grad.norm().item():.6f}\")\n",
    "\n",
    "# Extract gradients\n",
    "grads = acts_with_grad.get_grads()\n",
    "print(f\"\\nExtracted gradients:\")\n",
    "for key, val in grads.items():\n",
    "    if val is not None:\n",
    "        print(f\"  {key}: {val.shape}\")\n",
    "\n",
    "print(\"PASSED: Gradient computation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: stop_at_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an advanced feature for early stopping\n",
    "spec_dict = {\n",
    "    \"stop_at_layer\": 5,\n",
    "    \"activations\": {\n",
    "        \"positions\": -1,\n",
    "        \"locations\": [(0, \"layer_in\"), (3, \"attn\")],\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Extracting with stop_at_layer=5...\")\n",
    "with UnifiedAccessAndPatching(model, inputs, spec_dict) as uap:\n",
    "    activations, logits = uap.unified_access_and_patching()\n",
    "\n",
    "print(f\"Extracted keys: {list(activations.keys())}\")\n",
    "for key, val in activations.items():\n",
    "    print(f\"  {key}: {val.shape}\")\n",
    "print(\"PASSED: stop_at_layer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: create_z_patch_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get z activations from two different prompts\n",
    "prompt1 = tokenizer([\"Paris is the capital of France\"], thinking=False)\n",
    "prompt2 = tokenizer([\"Berlin is the capital of Germany\"], thinking=False)\n",
    "\n",
    "spec_dict = {\n",
    "    \"activations\": {\n",
    "        \"positions\": slice(None),\n",
    "        \"locations\": [(5, \"z\")],\n",
    "    }\n",
    "}\n",
    "\n",
    "with UnifiedAccessAndPatching(model, prompt1, spec_dict) as uap:\n",
    "    acts1, _ = uap.unified_access_and_patching()\n",
    "\n",
    "with UnifiedAccessAndPatching(model, prompt2, spec_dict) as uap:\n",
    "    acts2, _ = uap.unified_access_and_patching()\n",
    "\n",
    "print(f\"Acts1 z shape (fused): {acts1[(5, 'z')].shape}\")\n",
    "print(f\"Acts2 z shape (fused): {acts2[(5, 'z')].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split heads for patching\n",
    "acts1.split_heads()\n",
    "acts2.split_heads()\n",
    "\n",
    "print(f\"Acts1 z shape (split): {acts1[(5, 'z')].shape}\")\n",
    "print(f\"Acts2 z shape (split): {acts2[(5, 'z')].shape}\")\n",
    "\n",
    "# Create patch dict for specific layer-head pairs\n",
    "layer_head_pairs = [(5, 0), (5, 3)]  # Patch heads 0 and 3 at layer 5\n",
    "\n",
    "patch_dict = create_z_patch_dict(\n",
    "    original_acts=acts1,\n",
    "    new_acts=acts2,\n",
    "    layer_head=layer_head_pairs,\n",
    "    position=-1  # Patch last position only\n",
    ")\n",
    "\n",
    "print(f\"\\nPatch dict keys: {list(patch_dict.keys())}\")\n",
    "print(f\"Patch dict fused_heads: {patch_dict.fused_heads}\")\n",
    "for key, val in patch_dict.items():\n",
    "    print(f\"  {key}: {val.shape}\")\n",
    "\n",
    "print(\"PASSED: create_z_patch_dict()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Context manager cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "spec_dict = {\n",
    "    \"activations\": {\n",
    "        \"positions\": -1,\n",
    "        \"locations\": [(0, \"layer_in\")],\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run in context manager\n",
    "with UnifiedAccessAndPatching(model, inputs, spec_dict) as uap:\n",
    "    activations, logits = uap.unified_access_and_patching()\n",
    "    print(f\"Inside context: got {len(activations)} activations\")\n",
    "\n",
    "# After context exits, references should be cleaned up\n",
    "gc.collect()\n",
    "print(\"Context manager exited and cleanup completed\")\n",
    "print(\"PASSED: Context manager cleanup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Static method patch_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the static patch function\n",
    "original = torch.ones(2, 5, 10)\n",
    "new_value = torch.zeros(2, 1, 10)\n",
    "position = -1\n",
    "\n",
    "result = UnifiedAccessAndPatching.patch_fn(original, new_value, position)\n",
    "\n",
    "print(f\"Original last position sum: {original[:, -1, :].sum().item()}\")\n",
    "print(f\"Result last position sum: {result[:, -1, :].sum().item()}\")\n",
    "print(f\"Result other positions sum: {result[:, :-1, :].sum().item()}\")\n",
    "\n",
    "# Last position should be zeros, others should be ones\n",
    "assert result[:, -1, :].sum().item() == 0, \"Last position should be patched to zeros\"\n",
    "assert result[:, :-1, :].sum().item() == (2 * 4 * 10), \"Other positions should be unchanged\"\n",
    "print(\"PASSED: Static patch_fn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: metric_fn_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the example metric function\n",
    "acts = torch.randn(2, 5, 10)  # batch=2, pos=5, hidden=10\n",
    "result = UnifiedAccessAndPatching.metric_fn_example(acts)\n",
    "\n",
    "# Should sum the last position across batch and hidden dimensions\n",
    "expected = acts[:, -1, :].sum()\n",
    "print(f\"Metric result: {result.item():.4f}\")\n",
    "print(f\"Expected (acts[:, -1, :].sum()): {expected.item():.4f}\")\n",
    "\n",
    "assert torch.isclose(result, expected), \"metric_fn_example should sum last position\"\n",
    "print(\"PASSED: metric_fn_example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"All activations module tests PASSED!\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
