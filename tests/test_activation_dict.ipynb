{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test: activation_dict module\n",
    "\n",
    "This notebook tests `FrozenError`, `FreezableDict`, `ArithmeticOperation`, and `ActivationDict` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "import torch\n",
    "from transformers import AutoConfig\n",
    "from mech_interp_toolkit.activation_dict import (\n",
    "    FrozenError,\n",
    "    FreezableDict,\n",
    "    ArithmeticOperation,\n",
    "    ActivationDict,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Load model config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "print(f\"Loaded config for {model_name}\")\n",
    "print(f\"Hidden size: {config.hidden_size}\")\n",
    "print(f\"Num attention heads: {config.num_attention_heads}\")\n",
    "print(f\"Num hidden layers: {config.num_hidden_layers}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: FrozenError exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    raise FrozenError(\"Test error message\")\n",
    "except FrozenError as e:\n",
    "    print(f\"Caught FrozenError: {e}\")\n",
    "    assert \"Test error message\" in str(e)\n",
    "print(\"PASSED: FrozenError exception\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: ActivationDict initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic initialization\n",
    "act_dict = ActivationDict(config, positions=slice(None))\n",
    "\n",
    "print(f\"num_heads: {act_dict.num_heads}\")\n",
    "print(f\"num_layers: {act_dict.num_layers}\")\n",
    "print(f\"head_dim: {act_dict.head_dim}\")\n",
    "print(f\"model_dim: {act_dict.model_dim}\")\n",
    "print(f\"fused_heads: {act_dict.fused_heads}\")\n",
    "print(f\"value_type: {act_dict.value_type}\")\n",
    "\n",
    "assert act_dict.num_heads == config.num_attention_heads\n",
    "assert act_dict.num_layers == config.num_hidden_layers\n",
    "assert act_dict.head_dim == config.hidden_size // config.num_attention_heads\n",
    "assert act_dict.model_dim == config.hidden_size\n",
    "assert act_dict.fused_heads == True\n",
    "assert act_dict.value_type == \"activation\"\n",
    "print(\"PASSED: ActivationDict initialization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test initialization with different value_type\n",
    "grad_dict = ActivationDict(config, positions=-1, value_type=\"gradient\")\n",
    "assert grad_dict.value_type == \"gradient\"\n",
    "\n",
    "score_dict = ActivationDict(config, positions=[0, 1, 2], value_type=\"scores\")\n",
    "assert score_dict.value_type == \"scores\"\n",
    "print(\"PASSED: Different value_type initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Freeze/Unfreeze functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_dict = ActivationDict(config, positions=slice(None))\n",
    "\n",
    "# Add data before freezing\n",
    "act_dict[(0, \"attn\")] = torch.randn(2, 10, config.hidden_size)\n",
    "print(f\"Added (0, 'attn'): {act_dict[(0, 'attn')].shape}\")\n",
    "\n",
    "# Freeze the dictionary\n",
    "act_dict.freeze()\n",
    "assert act_dict._frozen == True, \"Should be frozen\"\n",
    "print(\"Dictionary frozen\")\n",
    "\n",
    "# Try to modify while frozen (should raise FrozenError)\n",
    "try:\n",
    "    act_dict[(0, \"mlp\")] = torch.randn(2, 10, config.hidden_size)\n",
    "    assert False, \"Should have raised FrozenError\"\n",
    "except FrozenError:\n",
    "    print(\"Correctly raised FrozenError on setitem\")\n",
    "\n",
    "# Try other modifying operations\n",
    "try:\n",
    "    del act_dict[(0, \"attn\")]\n",
    "    assert False, \"Should have raised FrozenError\"\n",
    "except FrozenError:\n",
    "    print(\"Correctly raised FrozenError on delitem\")\n",
    "\n",
    "try:\n",
    "    act_dict.clear()\n",
    "    assert False, \"Should have raised FrozenError\"\n",
    "except FrozenError:\n",
    "    print(\"Correctly raised FrozenError on clear\")\n",
    "\n",
    "# Unfreeze and modify\n",
    "act_dict.unfreeze()\n",
    "assert act_dict._frozen == False, \"Should be unfrozen\"\n",
    "act_dict[(0, \"mlp\")] = torch.randn(2, 10, config.hidden_size)\n",
    "print(f\"After unfreeze, added (0, 'mlp'): {act_dict[(0, 'mlp')].shape}\")\n",
    "print(\"PASSED: Freeze/Unfreeze functionality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original = ActivationDict(config, positions=slice(None))\n",
    "original[(0, \"attn\")] = torch.randn(2, 5, config.hidden_size)\n",
    "\n",
    "cloned = original.clone()\n",
    "\n",
    "# Verify clone has same data\n",
    "assert (0, \"attn\") in cloned, \"Cloned should have same keys\"\n",
    "assert torch.equal(original[(0, \"attn\")], cloned[(0, \"attn\")]), \"Data should be equal\"\n",
    "\n",
    "# Verify clone is independent (deep copy)\n",
    "cloned[(0, \"attn\")] = torch.zeros_like(cloned[(0, \"attn\")])\n",
    "assert not torch.equal(original[(0, \"attn\")], cloned[(0, \"attn\")]), \"Modifying clone should not affect original\"\n",
    "\n",
    "print(\"Original sum:\", original[(0, \"attn\")].sum().item())\n",
    "print(\"Cloned sum (after zeroing):\", cloned[(0, \"attn\")].sum().item())\n",
    "print(\"PASSED: clone()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Arithmetic operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two ActivationDicts with same keys\n",
    "act1 = ActivationDict(config, positions=slice(None))\n",
    "act1[(0, \"attn\")] = torch.ones(2, 5, config.hidden_size) * 3\n",
    "act1[(0, \"mlp\")] = torch.ones(2, 5, config.hidden_size) * 5\n",
    "\n",
    "act2 = ActivationDict(config, positions=slice(None))\n",
    "act2[(0, \"attn\")] = torch.ones(2, 5, config.hidden_size) * 2\n",
    "act2[(0, \"mlp\")] = torch.ones(2, 5, config.hidden_size) * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test addition\n",
    "result = act1 + act2\n",
    "assert torch.allclose(result[(0, \"attn\")], torch.ones(2, 5, config.hidden_size) * 5)\n",
    "assert torch.allclose(result[(0, \"mlp\")], torch.ones(2, 5, config.hidden_size) * 8)\n",
    "print(f\"Addition: 3 + 2 = {result[(0, 'attn')][0, 0, 0].item()}\")\n",
    "print(\"PASSED: Addition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test subtraction\n",
    "result = act1 - act2\n",
    "assert torch.allclose(result[(0, \"attn\")], torch.ones(2, 5, config.hidden_size) * 1)\n",
    "assert torch.allclose(result[(0, \"mlp\")], torch.ones(2, 5, config.hidden_size) * 2)\n",
    "print(f\"Subtraction: 3 - 2 = {result[(0, 'attn')][0, 0, 0].item()}\")\n",
    "print(\"PASSED: Subtraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiplication with ActivationDict\n",
    "result = act1 * act2\n",
    "assert torch.allclose(result[(0, \"attn\")], torch.ones(2, 5, config.hidden_size) * 6)\n",
    "assert torch.allclose(result[(0, \"mlp\")], torch.ones(2, 5, config.hidden_size) * 15)\n",
    "print(f\"Multiplication (ActivationDict): 3 * 2 = {result[(0, 'attn')][0, 0, 0].item()}\")\n",
    "print(\"PASSED: Multiplication (ActivationDict)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiplication with scalar\n",
    "result = act1 * 2.0\n",
    "assert torch.allclose(result[(0, \"attn\")], torch.ones(2, 5, config.hidden_size) * 6)\n",
    "print(f\"Multiplication (scalar): 3 * 2.0 = {result[(0, 'attn')][0, 0, 0].item()}\")\n",
    "\n",
    "# Test rmul\n",
    "result = 2.0 * act1\n",
    "assert torch.allclose(result[(0, \"attn\")], torch.ones(2, 5, config.hidden_size) * 6)\n",
    "print(f\"Reverse multiplication: 2.0 * 3 = {result[(0, 'attn')][0, 0, 0].item()}\")\n",
    "print(\"PASSED: Scalar multiplication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test division\n",
    "result = act1 / act2\n",
    "assert torch.allclose(result[(0, \"attn\")], torch.ones(2, 5, config.hidden_size) * 1.5)\n",
    "print(f\"Division (ActivationDict): 3 / 2 = {result[(0, 'attn')][0, 0, 0].item()}\")\n",
    "\n",
    "result = act1 / 2.0\n",
    "assert torch.allclose(result[(0, \"attn\")], torch.ones(2, 5, config.hidden_size) * 1.5)\n",
    "print(f\"Division (scalar): 3 / 2.0 = {result[(0, 'attn')][0, 0, 0].item()}\")\n",
    "print(\"PASSED: Division\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: split_heads() and merge_heads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_dict = ActivationDict(config, positions=slice(None))\n",
    "batch_size, seq_len = 2, 10\n",
    "# z activations with fused heads: (batch, pos, n_heads * d_head)\n",
    "act_dict[(0, \"z\")] = torch.randn(batch_size, seq_len, config.hidden_size)\n",
    "print(f\"Original z shape (fused): {act_dict[(0, 'z')].shape}\")\n",
    "assert act_dict.fused_heads == True\n",
    "\n",
    "# Split heads\n",
    "act_dict.split_heads()\n",
    "print(f\"After split_heads z shape: {act_dict[(0, 'z')].shape}\")\n",
    "expected_shape = (batch_size, seq_len, act_dict.num_heads, act_dict.head_dim)\n",
    "assert act_dict[(0, \"z\")].shape == expected_shape, f\"Expected {expected_shape}, got {act_dict[(0, 'z')].shape}\"\n",
    "assert act_dict.fused_heads == False\n",
    "print(\"PASSED: split_heads()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge heads back\n",
    "act_dict.merge_heads()\n",
    "print(f\"After merge_heads z shape: {act_dict[(0, 'z')].shape}\")\n",
    "expected_shape = (batch_size, seq_len, config.hidden_size)\n",
    "assert act_dict[(0, \"z\")].shape == expected_shape, f\"Expected {expected_shape}, got {act_dict[(0, 'z')].shape}\"\n",
    "assert act_dict.fused_heads == True\n",
    "print(\"PASSED: merge_heads()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_dict = ActivationDict(config, positions=slice(None))\n",
    "act_dict[(0, \"attn\")] = torch.ones(2, 5, config.hidden_size) * 3\n",
    "act_dict[(0, \"mlp\")] = torch.ones(2, 5, config.hidden_size) * 5\n",
    "\n",
    "# Apply sum along last dimension\n",
    "summed = act_dict.apply(torch.sum, dim=-1)\n",
    "print(f\"Original shape: {act_dict[(0, 'attn')].shape}\")\n",
    "print(f\"After apply(sum, dim=-1) shape: {summed[(0, 'attn')].shape}\")\n",
    "\n",
    "expected_sum = 3 * config.hidden_size\n",
    "assert torch.allclose(summed[(0, \"attn\")], torch.ones(2, 5) * expected_sum)\n",
    "print(f\"Sum value: {summed[(0, 'attn')][0, 0].item()} (expected {expected_sum})\")\n",
    "print(\"PASSED: apply()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: cuda() and cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_dict = ActivationDict(config, positions=slice(None))\n",
    "act_dict[(0, \"attn\")] = torch.randn(2, 5, config.hidden_size)\n",
    "\n",
    "# Test cpu()\n",
    "act_dict.cpu()\n",
    "assert act_dict[(0, \"attn\")].device.type == \"cpu\", \"Should be on CPU\"\n",
    "print(f\"Device after cpu(): {act_dict[(0, 'attn')].device}\")\n",
    "\n",
    "# Test cuda() if available\n",
    "if torch.cuda.is_available():\n",
    "    act_dict.cuda()\n",
    "    assert act_dict[(0, \"attn\")].device.type == \"cuda\", \"Should be on CUDA\"\n",
    "    print(f\"Device after cuda(): {act_dict[(0, 'attn')].device}\")\n",
    "    act_dict.cpu()  # Move back to CPU\n",
    "else:\n",
    "    print(\"CUDA not available, skipping cuda() test\")\n",
    "\n",
    "print(\"PASSED: cuda() and cpu()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: zeros_like()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_dict = ActivationDict(config, positions=slice(None))\n",
    "act_dict[(0, \"attn\")] = torch.randn(2, 5, config.hidden_size)\n",
    "act_dict[(0, \"mlp\")] = torch.randn(2, 5, config.hidden_size)\n",
    "\n",
    "# Create zeros for all keys\n",
    "zeros = act_dict.zeros_like()\n",
    "assert torch.all(zeros[(0, \"attn\")] == 0), \"Should be all zeros\"\n",
    "assert zeros[(0, \"attn\")].shape == act_dict[(0, \"attn\")].shape, \"Shape should match\"\n",
    "print(f\"zeros_like() sum: {zeros[(0, 'attn')].sum().item()}\")\n",
    "\n",
    "# Create zeros for specific keys\n",
    "zeros_partial = act_dict.zeros_like(keys=[(0, \"attn\")])\n",
    "assert (0, \"attn\") in zeros_partial, \"Should have requested key\"\n",
    "assert (0, \"mlp\") not in zeros_partial, \"Should not have unrequested key\"\n",
    "print(\"PASSED: zeros_like()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: reorganize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_dict = ActivationDict(config, positions=slice(None))\n",
    "# Add in random order\n",
    "act_dict[(1, \"mlp\")] = torch.randn(2, 5, config.hidden_size)\n",
    "act_dict[(0, \"z\")] = torch.randn(2, 5, config.hidden_size)\n",
    "act_dict[(0, \"layer_in\")] = torch.randn(2, 5, config.hidden_size)\n",
    "act_dict[(0, \"mlp\")] = torch.randn(2, 5, config.hidden_size)\n",
    "act_dict[(0, \"attn\")] = torch.randn(2, 5, config.hidden_size)\n",
    "\n",
    "print(f\"Original order: {list(act_dict.keys())}\")\n",
    "\n",
    "reorganized = act_dict.reorganize()\n",
    "print(f\"Reorganized order: {list(reorganized.keys())}\")\n",
    "\n",
    "# Expected order: (0, layer_in), (0, z), (0, attn), (0, mlp), (1, mlp)\n",
    "keys = list(reorganized.keys())\n",
    "assert keys[0] == (0, \"layer_in\"), \"First should be (0, 'layer_in')\"\n",
    "assert keys[1] == (0, \"z\"), \"Second should be (0, 'z')\"\n",
    "assert keys[2] == (0, \"attn\"), \"Third should be (0, 'attn')\"\n",
    "assert keys[3] == (0, \"mlp\"), \"Fourth should be (0, 'mlp')\"\n",
    "assert keys[4] == (1, \"mlp\"), \"Fifth should be (1, 'mlp')\"\n",
    "print(\"PASSED: reorganize()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: extract_positions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_dict = ActivationDict(config, positions=[1, 3])  # positions to extract\n",
    "act_dict[(0, \"attn\")] = torch.randn(2, 10, config.hidden_size)  # Full sequence\n",
    "\n",
    "extracted = act_dict.extract_positions()\n",
    "print(f\"Original shape: {act_dict[(0, 'attn')].shape}\")\n",
    "print(f\"Extracted shape: {extracted[(0, 'attn')].shape}\")\n",
    "\n",
    "# Should extract positions [1, 3] -> shape (batch, 2, hidden_size)\n",
    "assert extracted[(0, \"attn\")].shape == (2, 2, config.hidden_size)\n",
    "print(\"PASSED: extract_positions()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: get_grads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_dict = ActivationDict(config, positions=slice(None))\n",
    "tensor = torch.randn(2, 5, config.hidden_size, requires_grad=True)\n",
    "act_dict[(0, \"attn\")] = tensor\n",
    "\n",
    "# Compute some gradients\n",
    "loss = tensor.sum()\n",
    "loss.backward()\n",
    "\n",
    "print(f\"Gradient exists: {tensor.grad is not None}\")\n",
    "print(f\"Gradient shape: {tensor.grad.shape}\")\n",
    "\n",
    "# Get gradients\n",
    "grads = act_dict.get_grads()\n",
    "assert grads.value_type == \"gradient\", \"Should have gradient value_type\"\n",
    "assert grads[(0, \"attn\")] is not None, \"Should have gradient\"\n",
    "print(f\"Retrieved gradient shape: {grads[(0, 'attn')].shape}\")\n",
    "print(\"PASSED: get_grads()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"All activation_dict module tests PASSED!\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
