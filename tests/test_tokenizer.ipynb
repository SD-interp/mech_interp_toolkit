{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test: tokenizer module\n",
    "\n",
    "This notebook tests the `ChatTemplateTokenizer` class in `mech_interp_toolkit.tokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from mech_interp_toolkit.tokenizer import ChatTemplateTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Load base tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "print(f\"Loaded base tokenizer for {model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: ChatTemplateTokenizer initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test default initialization\n",
    "chat_tokenizer = ChatTemplateTokenizer(base_tokenizer)\n",
    "\n",
    "print(f\"System prompt: {chat_tokenizer.system_prompt}\")\n",
    "print(f\"Suffix: '{chat_tokenizer.suffix}'\")\n",
    "assert chat_tokenizer.tokenizer is not None, \"Tokenizer should be set\"\n",
    "assert chat_tokenizer.tokenizer.pad_token is not None, \"Pad token should be set\"\n",
    "print(f\"Pad token: {chat_tokenizer.tokenizer.pad_token}\")\n",
    "print(\"PASSED: Default initialization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test custom initialization\n",
    "custom_system_prompt = \"You are a helpful assistant.\"\n",
    "custom_suffix = \"\\n\\nThink step by step:\"\n",
    "\n",
    "custom_tokenizer = ChatTemplateTokenizer(\n",
    "    base_tokenizer,\n",
    "    system_prompt=custom_system_prompt,\n",
    "    suffix=custom_suffix\n",
    ")\n",
    "\n",
    "assert custom_tokenizer.system_prompt == custom_system_prompt, \"Custom system prompt not set\"\n",
    "assert custom_tokenizer.suffix == custom_suffix, \"Custom suffix not set\"\n",
    "print(f\"Custom system prompt: {custom_tokenizer.system_prompt}\")\n",
    "print(f\"Custom suffix: '{custom_tokenizer.suffix}'\")\n",
    "print(\"PASSED: Custom initialization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: _apply_chat_template()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with single string prompt\n",
    "single_prompt = \"What is 2 + 2?\"\n",
    "formatted = chat_tokenizer._apply_chat_template(single_prompt, thinking=False)\n",
    "\n",
    "print(f\"Single prompt formatted:\")\n",
    "print(formatted[0][:200] + \"...\")\n",
    "print()\n",
    "\n",
    "assert isinstance(formatted, list), \"Should return a list\"\n",
    "assert len(formatted) == 1, \"Should have one formatted prompt\"\n",
    "assert chat_tokenizer.system_prompt in formatted[0], \"System prompt should be in formatted text\"\n",
    "assert single_prompt in formatted[0], \"User prompt should be in formatted text\"\n",
    "print(\"PASSED: Single string prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with list of prompts\n",
    "multiple_prompts = [\n",
    "    \"What is the capital of France?\",\n",
    "    \"What is 5 * 7?\",\n",
    "    \"Explain gravity.\"\n",
    "]\n",
    "\n",
    "formatted = chat_tokenizer._apply_chat_template(multiple_prompts, thinking=False)\n",
    "\n",
    "print(f\"Number of formatted prompts: {len(formatted)}\")\n",
    "assert len(formatted) == 3, \"Should have three formatted prompts\"\n",
    "\n",
    "for i, (orig, fmt) in enumerate(zip(multiple_prompts, formatted)):\n",
    "    assert orig in fmt, f\"Original prompt {i} should be in formatted text\"\n",
    "    print(f\"Prompt {i}: '{orig}' -> {len(fmt)} chars\")\n",
    "\n",
    "print(\"PASSED: List of prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with thinking=True\n",
    "prompt = \"Solve this puzzle\"\n",
    "formatted_thinking = chat_tokenizer._apply_chat_template(prompt, thinking=True)\n",
    "formatted_no_thinking = chat_tokenizer._apply_chat_template(prompt, thinking=False)\n",
    "\n",
    "print(f\"With thinking=True: {len(formatted_thinking[0])} chars\")\n",
    "print(f\"With thinking=False: {len(formatted_no_thinking[0])} chars\")\n",
    "# The formats may differ depending on the model's chat template\n",
    "print(\"PASSED: thinking parameter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: _encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test encoding single formatted prompt\n",
    "formatted = chat_tokenizer._apply_chat_template(\"Hello, world!\", thinking=False)\n",
    "encoded = chat_tokenizer._encode(formatted)\n",
    "\n",
    "print(f\"Encoded keys: {encoded.keys()}\")\n",
    "print(f\"input_ids shape: {encoded['input_ids'].shape}\")\n",
    "print(f\"attention_mask shape: {encoded['attention_mask'].shape}\")\n",
    "\n",
    "assert \"input_ids\" in encoded, \"Should have input_ids\"\n",
    "assert \"attention_mask\" in encoded, \"Should have attention_mask\"\n",
    "assert isinstance(encoded[\"input_ids\"], torch.Tensor), \"input_ids should be a tensor\"\n",
    "assert isinstance(encoded[\"attention_mask\"], torch.Tensor), \"attention_mask should be a tensor\"\n",
    "print(\"PASSED: Single prompt encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test encoding multiple prompts with padding\n",
    "prompts = [\n",
    "    \"Short\",\n",
    "    \"This is a much longer prompt that should require more tokens\"\n",
    "]\n",
    "formatted = chat_tokenizer._apply_chat_template(prompts, thinking=False)\n",
    "encoded = chat_tokenizer._encode(formatted)\n",
    "\n",
    "print(f\"Batch input_ids shape: {encoded['input_ids'].shape}\")\n",
    "print(f\"Batch attention_mask shape: {encoded['attention_mask'].shape}\")\n",
    "\n",
    "batch_size, seq_len = encoded[\"input_ids\"].shape\n",
    "assert batch_size == 2, \"Batch size should be 2\"\n",
    "assert encoded[\"input_ids\"].shape == encoded[\"attention_mask\"].shape, \"Shapes should match\"\n",
    "\n",
    "# Check padding (left padding by default)\n",
    "print(f\"First sequence attention mask: {encoded['attention_mask'][0, :10].tolist()}...\")\n",
    "print(f\"Second sequence attention mask: {encoded['attention_mask'][1, :10].tolist()}...\")\n",
    "print(\"PASSED: Batch encoding with padding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: __call__() (main interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test direct call with single prompt\n",
    "result = chat_tokenizer(\"What is machine learning?\", thinking=False)\n",
    "\n",
    "print(f\"Result type: {type(result)}\")\n",
    "print(f\"Keys: {result.keys()}\")\n",
    "print(f\"input_ids shape: {result['input_ids'].shape}\")\n",
    "print(f\"attention_mask shape: {result['attention_mask'].shape}\")\n",
    "\n",
    "assert \"input_ids\" in result, \"Should have input_ids\"\n",
    "assert \"attention_mask\" in result, \"Should have attention_mask\"\n",
    "assert result[\"input_ids\"].dim() == 2, \"Should be 2D tensor (batch, seq_len)\"\n",
    "print(\"PASSED: Single prompt call\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test direct call with multiple prompts\n",
    "prompts = [\n",
    "    \"Define artificial intelligence.\",\n",
    "    \"What is deep learning?\",\n",
    "    \"Explain neural networks.\",\n",
    "    \"What is NLP?\"\n",
    "]\n",
    "\n",
    "result = chat_tokenizer(prompts, thinking=False)\n",
    "\n",
    "print(f\"Batch size: {result['input_ids'].shape[0]}\")\n",
    "print(f\"Sequence length: {result['input_ids'].shape[1]}\")\n",
    "\n",
    "assert result[\"input_ids\"].shape[0] == 4, \"Batch size should be 4\"\n",
    "print(\"PASSED: Multiple prompts call\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify structured_prompt is stored\n",
    "chat_tokenizer(\"Test prompt\", thinking=False)\n",
    "\n",
    "assert chat_tokenizer.structured_prompt is not None, \"structured_prompt should be stored\"\n",
    "print(f\"Stored structured prompt (truncated): {chat_tokenizer.structured_prompt[0][:100]}...\")\n",
    "print(\"PASSED: structured_prompt storage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Suffix functionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that suffix is appended\n",
    "suffix = \"\\n\\nAnswer:\"\n",
    "tokenizer_with_suffix = ChatTemplateTokenizer(base_tokenizer, suffix=suffix)\n",
    "\n",
    "formatted = tokenizer_with_suffix._apply_chat_template(\"Question?\", thinking=False)\n",
    "\n",
    "assert formatted[0].endswith(suffix), f\"Formatted prompt should end with suffix. Got: ...{formatted[0][-50:]}\"\n",
    "print(f\"Formatted ends with suffix: {formatted[0][-30:]}\")\n",
    "print(\"PASSED: Suffix functionality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"All tokenizer module tests PASSED!\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
