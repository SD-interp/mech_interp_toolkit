{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test: misc module\n",
    "\n",
    "This notebook tests the `get_attention_pattern` function from `mech_interp_toolkit.misc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "import torch\n",
    "from mech_interp_toolkit.utils import load_model_tokenizer_config, get_default_device\n",
    "from mech_interp_toolkit.misc import get_attention_pattern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Load model with eager attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "device = get_default_device()\n",
    "\n",
    "# Load with eager attention to get attention patterns\n",
    "print(f\"Loading model {model_name} with eager attention on {device}...\")\n",
    "model, tokenizer, config = load_model_tokenizer_config(\n",
    "    model_name, \n",
    "    device=device,\n",
    "    attn_type=\"eager\"  # Required for attention pattern extraction\n",
    ")\n",
    "print(\"Model loaded successfully\")\n",
    "print(f\"Number of layers: {config.num_hidden_layers}\")\n",
    "print(f\"Number of attention heads: {config.num_attention_heads}\")\n",
    "print(f\"Attention implementation: {config._attn_implementation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test inputs\n",
    "prompts = [\"The quick brown fox jumps over the lazy dog.\"]\n",
    "inputs = tokenizer(prompts, thinking=False)\n",
    "\n",
    "print(f\"Input IDs shape: {inputs['input_ids'].shape}\")\n",
    "seq_len = inputs['input_ids'].shape[1]\n",
    "print(f\"Sequence length: {seq_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Basic attention pattern extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract attention patterns for specific layers and heads\n",
    "layers = [0, 5]\n",
    "head_indices = [[0, 1], [2, 3]]  # Heads for each layer\n",
    "\n",
    "print(f\"Extracting attention patterns for layers {layers}...\")\n",
    "attn_patterns = get_attention_pattern(\n",
    "    model=model,\n",
    "    inputs=inputs,\n",
    "    layers=layers,\n",
    "    head_indices=head_indices,\n",
    "    query_position=-1,  # Last position\n",
    ")\n",
    "\n",
    "print(f\"\\nExtracted patterns for {len(attn_patterns)} layers\")\n",
    "for layer, pattern in attn_patterns.items():\n",
    "    print(f\"  Layer {layer}: shape = {pattern.shape}\")\n",
    "\n",
    "# Check shapes\n",
    "# Expected: (batch, selected_heads, seq_len) for query_position=-1\n",
    "assert 0 in attn_patterns, \"Should have layer 0\"\n",
    "assert 5 in attn_patterns, \"Should have layer 5\"\n",
    "\n",
    "print(\"PASSED: Basic attention pattern extraction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze attention pattern\n",
    "pattern_0 = attn_patterns[0]\n",
    "print(f\"Layer 0 attention pattern shape: {pattern_0.shape}\")\n",
    "print(f\"Pattern sum per head (should be close to 1.0):\")\n",
    "\n",
    "for head_idx in range(pattern_0.shape[1]):\n",
    "    head_sum = pattern_0[0, head_idx, :].sum().item()\n",
    "    print(f\"  Head {head_indices[0][head_idx]}: sum = {head_sum:.4f}\")\n",
    "\n",
    "# Attention should sum to approximately 1.0\n",
    "assert abs(pattern_0[0, 0, :].sum().item() - 1.0) < 0.1, \"Attention should sum to ~1.0\"\n",
    "print(\"PASSED: Attention pattern sums to ~1.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Single layer, single head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract single head attention\n",
    "attn_single = get_attention_pattern(\n",
    "    model=model,\n",
    "    inputs=inputs,\n",
    "    layers=[10],\n",
    "    head_indices=[[5]],  # Single head\n",
    "    query_position=-1,\n",
    ")\n",
    "\n",
    "print(f\"Single head pattern shape: {attn_single[10].shape}\")\n",
    "assert attn_single[10].shape[1] == 1, \"Should have single head\"\n",
    "print(\"PASSED: Single head extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Multiple heads per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all heads for a single layer\n",
    "n_heads = config.num_attention_heads\n",
    "all_heads = list(range(n_heads))\n",
    "\n",
    "print(f\"Extracting all {n_heads} heads for layer 0...\")\n",
    "attn_all = get_attention_pattern(\n",
    "    model=model,\n",
    "    inputs=inputs,\n",
    "    layers=[0],\n",
    "    head_indices=[all_heads],\n",
    "    query_position=-1,\n",
    ")\n",
    "\n",
    "print(f\"All heads pattern shape: {attn_all[0].shape}\")\n",
    "assert attn_all[0].shape[1] == n_heads, f\"Should have {n_heads} heads\"\n",
    "print(\"PASSED: All heads extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Different query positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different query positions\n",
    "layers = [0]\n",
    "head_indices = [[0, 1, 2]]\n",
    "\n",
    "print(\"Testing different query positions:\")\n",
    "for query_pos in [-1, 0, seq_len // 2]:\n",
    "    attn = get_attention_pattern(\n",
    "        model=model,\n",
    "        inputs=inputs,\n",
    "        layers=layers,\n",
    "        head_indices=head_indices,\n",
    "        query_position=query_pos,\n",
    "    )\n",
    "    print(f\"  query_position={query_pos}: shape = {attn[0].shape}\")\n",
    "\n",
    "print(\"PASSED: Different query positions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Error handling - mismatched layers and heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test error when layers and head_indices don't match\n",
    "try:\n",
    "    attn = get_attention_pattern(\n",
    "        model=model,\n",
    "        inputs=inputs,\n",
    "        layers=[0, 1, 2],  # 3 layers\n",
    "        head_indices=[[0], [1]],  # Only 2 head lists\n",
    "        query_position=-1,\n",
    "    )\n",
    "    assert False, \"Should have raised ValueError\"\n",
    "except ValueError as e:\n",
    "    print(f\"Correctly raised error: {e}\")\n",
    "\n",
    "print(\"PASSED: Mismatch error handling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Attention pattern visualization data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get attention data that could be used for visualization\n",
    "layers = [0, 10, config.num_hidden_layers - 1]\n",
    "head_indices = [[0], [0], [0]]  # Head 0 for each layer\n",
    "\n",
    "print(\"Extracting attention patterns across layers...\")\n",
    "attn_viz = get_attention_pattern(\n",
    "    model=model,\n",
    "    inputs=inputs,\n",
    "    layers=layers,\n",
    "    head_indices=head_indices,\n",
    "    query_position=-1,\n",
    ")\n",
    "\n",
    "print(\"\\nAttention weights from last token (query) to all tokens (keys):\")\n",
    "for layer in layers:\n",
    "    pattern = attn_viz[layer][0, 0, :].cpu().numpy()  # First batch, first head\n",
    "    top_5_idx = pattern.argsort()[-5:][::-1]  # Top 5 attended positions\n",
    "    print(f\"\\nLayer {layer}:\")\n",
    "    print(f\"  Pattern shape: {attn_viz[layer].shape}\")\n",
    "    print(f\"  Top 5 positions: {top_5_idx}\")\n",
    "    print(f\"  Top 5 weights: {pattern[top_5_idx]}\")\n",
    "\n",
    "print(\"\\nPASSED: Attention pattern analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: With batch of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with multiple prompts\n",
    "batch_prompts = [\n",
    "    \"Hello world!\",\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"Machine learning is fascinating.\"\n",
    "]\n",
    "batch_inputs = tokenizer(batch_prompts, thinking=False)\n",
    "\n",
    "print(f\"Batch input shape: {batch_inputs['input_ids'].shape}\")\n",
    "\n",
    "attn_batch = get_attention_pattern(\n",
    "    model=model,\n",
    "    inputs=batch_inputs,\n",
    "    layers=[5],\n",
    "    head_indices=[[0, 1]],\n",
    "    query_position=-1,\n",
    ")\n",
    "\n",
    "print(f\"Batch attention pattern shape: {attn_batch[5].shape}\")\n",
    "# Should be (batch_size, num_heads, seq_len)\n",
    "assert attn_batch[5].shape[0] == 3, \"Should have batch size 3\"\n",
    "print(\"PASSED: Batch attention extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Warning for non-eager attention (informational)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model with non-eager attention to see warning\n",
    "import warnings\n",
    "\n",
    "print(\"Loading model with SDPA attention...\")\n",
    "model_sdpa, tok_sdpa, cfg_sdpa = load_model_tokenizer_config(\n",
    "    model_name,\n",
    "    device=device,\n",
    "    attn_type=\"sdpa\"\n",
    ")\n",
    "print(f\"Attention implementation: {cfg_sdpa._attn_implementation}\")\n",
    "\n",
    "# This should emit a warning\n",
    "print(\"\\nAttempting to get attention patterns (may emit warning)...\")\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    warnings.simplefilter(\"always\")\n",
    "    try:\n",
    "        attn = get_attention_pattern(\n",
    "            model=model_sdpa,\n",
    "            inputs=inputs,\n",
    "            layers=[0],\n",
    "            head_indices=[[0]],\n",
    "            query_position=-1,\n",
    "        )\n",
    "        if len(w) > 0:\n",
    "            print(f\"Warning emitted: {w[-1].message}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error (expected for non-eager): {e}\")\n",
    "\n",
    "print(\"PASSED: Non-eager attention handling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"All misc module tests PASSED!\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
