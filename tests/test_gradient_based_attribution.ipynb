{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !git clone https://github.com/SD-interp/mech_interp_toolkit.git\n",
        "# %cd mech_interp_toolkit\n",
        "# !pip install -e ."
      ],
      "metadata": {
        "id": "a6RCF-4YhrNW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EyhEO1-qhmR6"
      },
      "source": [
        "# Test: gradient_based_attribution module\n",
        "\n",
        "This notebook tests gradient-based attribution methods from `mech_interp_toolkit.gradient_based_attribution`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "QsCOxZgLhmR8"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath(\"../src\"))\n",
        "\n",
        "import torch\n",
        "from mech_interp_toolkit.utils import load_model_tokenizer_config, get_default_device, get_all_layer_components\n",
        "from mech_interp_toolkit.activation_dict import ActivationDict\n",
        "from mech_interp_toolkit.gradient_based_attribution import (\n",
        "    edge_attribution_patching,\n",
        "    simple_integrated_gradients,\n",
        "    eap_integrated_gradients,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYNV6lK8hmR9"
      },
      "source": [
        "## Setup: Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "B9FiYbilhmR9",
        "outputId": "0a50006b-3630-42cb-fbaf-06b3c060aa4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model Qwen/Qwen3-0.6B on cuda...\n",
            "Model loaded successfully\n",
            "Number of layers: 28\n",
            "Hidden size: 1024\n"
          ]
        }
      ],
      "source": [
        "model_name = \"Qwen/Qwen3-0.6B\"\n",
        "device = get_default_device()\n",
        "\n",
        "print(f\"Loading model {model_name} on {device}...\")\n",
        "model, tokenizer, config = load_model_tokenizer_config(model_name, device=device)\n",
        "print(\"Model loaded successfully\")\n",
        "print(f\"Number of layers: {config.num_hidden_layers}\")\n",
        "print(f\"Hidden size: {config.hidden_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "pDgADwhDhmR9",
        "outputId": "67c7df78-0ca0-4720-bead-123dbad15553",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clean input shape: torch.Size([1, 33])\n",
            "Corrupted input shape: torch.Size([1, 33])\n"
          ]
        }
      ],
      "source": [
        "# Prepare clean and corrupted inputs for EAP\n",
        "clean_prompts = [\"The capital of France is\"]\n",
        "corrupted_prompts = [\"The capital of Germany is\"]\n",
        "\n",
        "clean_inputs = tokenizer(clean_prompts, thinking=False)\n",
        "corrupted_inputs = tokenizer(corrupted_prompts, thinking=False)\n",
        "\n",
        "print(f\"Clean input shape: {clean_inputs['input_ids'].shape}\")\n",
        "print(f\"Corrupted input shape: {corrupted_inputs['input_ids'].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIm4JrcehmR9"
      },
      "source": [
        "## Test: edge_attribution_patching()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WyWwhcOChmR9",
        "outputId": "b3f3ab95-29b3-4156-80bf-361bf97be6da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Edge Attribution Patching (EAP)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/mech_interp_toolkit/src/mech_interp_toolkit/activations.py:144: UserWarning: slicing and indexing when capturing gradients is not supported.\n",
            "                    All positions will be captured by default. Use output.extract_positions() instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(0, 'attn') >> True\n",
            "(0, 'mlp') >> True\n",
            "(1, 'attn') >> True\n",
            "(1, 'mlp') >> True\n",
            "(2, 'attn') >> True\n",
            "(2, 'mlp') >> True\n",
            "(3, 'attn') >> True\n",
            "(3, 'mlp') >> True\n",
            "(4, 'attn') >> True\n",
            "(4, 'mlp') >> True\n",
            "(5, 'attn') >> True\n",
            "(5, 'mlp') >> True\n",
            "(6, 'attn') >> True\n",
            "(6, 'mlp') >> True\n",
            "(7, 'attn') >> True\n",
            "(7, 'mlp') >> True\n",
            "(8, 'attn') >> True\n",
            "(8, 'mlp') >> True\n",
            "(9, 'attn') >> True\n",
            "(9, 'mlp') >> True\n",
            "(10, 'attn') >> True\n",
            "(10, 'mlp') >> True\n",
            "(11, 'attn') >> True\n",
            "(11, 'mlp') >> True\n",
            "(12, 'attn') >> True\n",
            "(12, 'mlp') >> True\n",
            "(13, 'attn') >> True\n",
            "(13, 'mlp') >> True\n",
            "(14, 'attn') >> True\n",
            "(14, 'mlp') >> True\n",
            "(15, 'attn') >> True\n",
            "(15, 'mlp') >> True\n",
            "(16, 'attn') >> True\n",
            "(16, 'mlp') >> True\n",
            "(17, 'attn') >> True\n",
            "(17, 'mlp') >> True\n",
            "(18, 'attn') >> True\n",
            "(18, 'mlp') >> True\n",
            "(19, 'attn') >> True\n",
            "(19, 'mlp') >> True\n",
            "(20, 'attn') >> True\n",
            "(20, 'mlp') >> True\n",
            "(21, 'attn') >> True\n",
            "(21, 'mlp') >> True\n",
            "(22, 'attn') >> True\n",
            "(22, 'mlp') >> True\n",
            "(23, 'attn') >> True\n",
            "(23, 'mlp') >> True\n",
            "(24, 'attn') >> True\n",
            "(24, 'mlp') >> True\n",
            "(25, 'attn') >> True\n",
            "(25, 'mlp') >> True\n",
            "(26, 'attn') >> True\n",
            "(26, 'mlp') >> True\n",
            "(27, 'attn') >> True\n",
            "(27, 'mlp') >> True\n",
            "\n",
            "EAP scores computed for 56 components\n",
            "EAP score type: <class 'mech_interp_toolkit.activation_dict.ActivationDict'>\n",
            "\n",
            "Sample EAP scores:\n",
            "  (0, 'attn'): 0.10343898832798004\n",
            "  (0, 'mlp'): 0.10243628919124603\n",
            "  (1, 'attn'): 0.020722653716802597\n",
            "  (1, 'mlp'): 0.03407488018274307\n",
            "  (2, 'attn'): -0.022822650149464607\n",
            "  (2, 'mlp'): -0.03263003006577492\n"
          ]
        }
      ],
      "source": [
        "# Define a simple metric function\n",
        "def metric_fn(logits):\n",
        "    return logits[:, -1, :].max(dim=-1).values.sum()\n",
        "\n",
        "print(\"Running Edge Attribution Patching (EAP)...\")\n",
        "eap_scores = edge_attribution_patching(\n",
        "    model=model,\n",
        "    clean_inputs=clean_inputs,\n",
        "    corrupted_inputs=corrupted_inputs,\n",
        "    compute_grad_at=\"clean\",\n",
        "    metric_fn=metric_fn,\n",
        "    position=-1,\n",
        ")\n",
        "\n",
        "print(f\"\\nEAP scores computed for {len(eap_scores)} components\")\n",
        "print(f\"EAP score type: {type(eap_scores)}\")\n",
        "print(f\"\\nSample EAP scores:\")\n",
        "for key, score in list(eap_scores.items())[:6]:\n",
        "    print(f\"  {key}: {score}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "udfGaTQJhmR-",
        "outputId": "fa7c8428-cfc2-45f3-b759-500b8240f4e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "EAP scores summary by layer:\n",
            "Layer            Attn          MLP\n",
            "--------------------------------\n",
            "0            0.103439     0.102436\n",
            "1            0.020723     0.034075\n",
            "2           -0.022823    -0.032630\n",
            "3            0.016342     0.007022\n",
            "4           -0.014269    -0.012199\n",
            "5            0.033902     0.028130\n",
            "6           -0.005734    -0.002962\n",
            "7           -0.001622     0.007245\n",
            "8            0.011429    -0.044762\n",
            "9           -0.005803    -0.013749\n",
            "10          -0.004995     0.040908\n",
            "11          -0.086160     0.022783\n",
            "12          -0.005039    -0.021428\n",
            "13          -0.013774     0.034420\n",
            "14          -0.020320    -0.038764\n",
            "15           0.009910    -0.008491\n",
            "16          -0.000997    -0.026708\n",
            "17           0.038213     0.061616\n",
            "18          -0.006833    -0.009788\n",
            "19           0.007158    -0.010926\n",
            "20          -0.011855    -0.019960\n",
            "21           0.008507    -0.054610\n",
            "22          -0.000294     0.004383\n",
            "23          -0.010230    -0.017337\n",
            "24           0.004418    -0.006846\n",
            "25          -0.001784     0.019974\n",
            "26          -0.013292     0.013549\n",
            "27          -0.012554     0.024031\n",
            "PASSED: Edge Attribution Patching\n"
          ]
        }
      ],
      "source": [
        "# Analyze EAP scores\n",
        "n_layers = config.num_hidden_layers\n",
        "\n",
        "print(\"\\nEAP scores summary by layer:\")\n",
        "print(f\"{'Layer':<8} {'Attn':>12} {'MLP':>12}\")\n",
        "print(\"-\" * 32)\n",
        "\n",
        "for layer in range(n_layers):\n",
        "    attn_key = (layer, \"attn\")\n",
        "    mlp_key = (layer, \"mlp\")\n",
        "\n",
        "    attn_score = eap_scores.get(attn_key, torch.tensor(0.0)).item() if attn_key in eap_scores else 0.0\n",
        "    mlp_score = eap_scores.get(mlp_key, torch.tensor(0.0)).item() if mlp_key in eap_scores else 0.0\n",
        "\n",
        "    print(f\"{layer:<8} {attn_score:>12.6f} {mlp_score:>12.6f}\")\n",
        "\n",
        "print(\"PASSED: Edge Attribution Patching\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WQI3hfvzhmR-",
        "outputId": "063a42e1-2db1-492e-89bd-99928e718c3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running EAP with compute_grad_at='corrupted'...\n",
            "(0, 'attn') >> True\n",
            "(0, 'mlp') >> True\n",
            "(1, 'attn') >> True\n",
            "(1, 'mlp') >> True\n",
            "(2, 'attn') >> True\n",
            "(2, 'mlp') >> True\n",
            "(3, 'attn') >> True\n",
            "(3, 'mlp') >> True\n",
            "(4, 'attn') >> True\n",
            "(4, 'mlp') >> True\n",
            "(5, 'attn') >> True\n",
            "(5, 'mlp') >> True\n",
            "(6, 'attn') >> True\n",
            "(6, 'mlp') >> True\n",
            "(7, 'attn') >> True\n",
            "(7, 'mlp') >> True\n",
            "(8, 'attn') >> True\n",
            "(8, 'mlp') >> True\n",
            "(9, 'attn') >> True\n",
            "(9, 'mlp') >> True\n",
            "(10, 'attn') >> True\n",
            "(10, 'mlp') >> True\n",
            "(11, 'attn') >> True\n",
            "(11, 'mlp') >> True\n",
            "(12, 'attn') >> True\n",
            "(12, 'mlp') >> True\n",
            "(13, 'attn') >> True\n",
            "(13, 'mlp') >> True\n",
            "(14, 'attn') >> True\n",
            "(14, 'mlp') >> True\n",
            "(15, 'attn') >> True\n",
            "(15, 'mlp') >> True\n",
            "(16, 'attn') >> True\n",
            "(16, 'mlp') >> True\n",
            "(17, 'attn') >> True\n",
            "(17, 'mlp') >> True\n",
            "(18, 'attn') >> True\n",
            "(18, 'mlp') >> True\n",
            "(19, 'attn') >> True\n",
            "(19, 'mlp') >> True\n",
            "(20, 'attn') >> True\n",
            "(20, 'mlp') >> True\n",
            "(21, 'attn') >> True\n",
            "(21, 'mlp') >> True\n",
            "(22, 'attn') >> True\n",
            "(22, 'mlp') >> True\n",
            "(23, 'attn') >> True\n",
            "(23, 'mlp') >> True\n",
            "(24, 'attn') >> True\n",
            "(24, 'mlp') >> True\n",
            "(25, 'attn') >> True\n",
            "(25, 'mlp') >> True\n",
            "(26, 'attn') >> True\n",
            "(26, 'mlp') >> True\n",
            "(27, 'attn') >> True\n",
            "(27, 'mlp') >> True\n",
            "EAP scores (corrupted): 56 components\n",
            "Sample scores:\n",
            "  (0, 'attn'): -0.1081891804933548\n",
            "  (0, 'mlp'): -0.12108974903821945\n",
            "  (1, 'attn'): -0.02913277968764305\n",
            "PASSED: EAP with corrupted gradients\n"
          ]
        }
      ],
      "source": [
        "# Test with compute_grad_at=\"corrupted\"\n",
        "print(\"\\nRunning EAP with compute_grad_at='corrupted'...\")\n",
        "eap_scores_corrupted = edge_attribution_patching(\n",
        "    model=model,\n",
        "    clean_inputs=clean_inputs,\n",
        "    corrupted_inputs=corrupted_inputs,\n",
        "    compute_grad_at=\"corrupted\",\n",
        "    metric_fn=metric_fn,\n",
        "    position=-1,\n",
        ")\n",
        "\n",
        "print(f\"EAP scores (corrupted): {len(eap_scores_corrupted)} components\")\n",
        "print(\"Sample scores:\")\n",
        "for key in list(eap_scores_corrupted.keys())[:3]:\n",
        "    print(f\"  {key}: {eap_scores_corrupted[key]}\")\n",
        "\n",
        "print(\"PASSED: EAP with corrupted gradients\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHlXAZIzhmR-"
      },
      "source": [
        "## Test: simple_integrated_gradients()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "I6Q4mcB3hmR-",
        "outputId": "0cfcfdaf-646d-4894-e556-9db100342b61",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sequence length: 32\n",
            "Baseline embeddings shape: torch.Size([1, 32, 1024])\n"
          ]
        }
      ],
      "source": [
        "# Create baseline embeddings (zeros)\n",
        "inputs = tokenizer([\"The quick brown fox\"], thinking=False)\n",
        "seq_len = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "baseline_embeddings = ActivationDict(config, positions=slice(None))\n",
        "baseline_embeddings[(0, \"layer_in\")] = torch.zeros(\n",
        "    1, seq_len, config.hidden_size,\n",
        "    device=device,\n",
        "    dtype=model.dtype\n",
        ")\n",
        "\n",
        "print(f\"Input sequence length: {seq_len}\")\n",
        "print(f\"Baseline embeddings shape: {baseline_embeddings[(0, 'layer_in')].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hOI85XAChmR-",
        "outputId": "60177ce3-9e9f-4a39-edaa-5ff169db1c9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Simple Integrated Gradients...\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "\n",
            "IG attributions computed\n",
            "Keys: [(0, 'layer_in')]\n",
            "  (0, 'layer_in'): shape=torch.Size([1, 32])\n",
            "PASSED: Simple Integrated Gradients\n"
          ]
        }
      ],
      "source": [
        "print(\"Running Simple Integrated Gradients...\")\n",
        "ig_attributions = simple_integrated_gradients(\n",
        "    model=model,\n",
        "    inputs=inputs,\n",
        "    baseline_embeddings=baseline_embeddings,\n",
        "    metric_fn=metric_fn,\n",
        "    steps=10,  # Using fewer steps for faster testing\n",
        ")\n",
        "\n",
        "print(f\"\\nIG attributions computed\")\n",
        "print(f\"Keys: {list(ig_attributions.keys())}\")\n",
        "\n",
        "for key, val in ig_attributions.items():\n",
        "    print(f\"  {key}: shape={val.shape}\")\n",
        "\n",
        "print(\"PASSED: Simple Integrated Gradients\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "r2i58XHMhmR_",
        "outputId": "ebd5dd0c-2390-449c-cce3-cf6cef6849d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "IG attribution shape: torch.Size([1, 32])\n",
            "\n",
            "IG attribution per position:\n",
            "  Position 0: nan\n",
            "  Position 1: nan\n",
            "  Position 2: nan\n",
            "  Position 3: nan\n",
            "  Position 4: nan\n",
            "  Position 5: nan\n",
            "  Position 6: nan\n",
            "  Position 7: nan\n",
            "  Position 8: nan\n",
            "  Position 9: nan\n",
            "  Position 10: nan\n",
            "  Position 11: nan\n",
            "  Position 12: nan\n",
            "  Position 13: nan\n",
            "  Position 14: nan\n",
            "  Position 15: nan\n",
            "  Position 16: nan\n",
            "  Position 17: nan\n",
            "  Position 18: nan\n",
            "  Position 19: nan\n",
            "  Position 20: nan\n",
            "  Position 21: nan\n",
            "  Position 22: nan\n",
            "  Position 23: nan\n",
            "  Position 24: nan\n",
            "  Position 25: nan\n",
            "  Position 26: nan\n",
            "  Position 27: nan\n",
            "  Position 28: nan\n",
            "  Position 29: nan\n",
            "  Position 30: nan\n",
            "  Position 31: nan\n"
          ]
        }
      ],
      "source": [
        "# Analyze IG attributions per position\n",
        "ig_values = ig_attributions[(0, \"layer_in\")]\n",
        "print(f\"\\nIG attribution shape: {ig_values.shape}\")\n",
        "print(f\"\\nIG attribution per position:\")\n",
        "\n",
        "for pos in range(ig_values.shape[1]):\n",
        "    print(f\"  Position {pos}: {ig_values[0, pos].item():.6f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "_bNYXI5fhmR_",
        "outputId": "cd3d3d87-3b33-4baa-8f96-4202130b1b85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Testing IG with different step counts:\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "  Steps=5: total attribution = nan\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "  Steps=10: total attribution = nan\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "(0, 'layer_in') >> True\n",
            "  Steps=20: total attribution = nan\n",
            "PASSED: IG with different step counts\n"
          ]
        }
      ],
      "source": [
        "# Test with different step counts\n",
        "print(\"\\nTesting IG with different step counts:\")\n",
        "for steps in [5, 10, 20]:\n",
        "    ig = simple_integrated_gradients(\n",
        "        model=model,\n",
        "        inputs=inputs,\n",
        "        baseline_embeddings=baseline_embeddings,\n",
        "        metric_fn=metric_fn,\n",
        "        steps=steps,\n",
        "    )\n",
        "    total_attribution = ig[(0, \"layer_in\")].sum().item()\n",
        "    print(f\"  Steps={steps}: total attribution = {total_attribution:.6f}\")\n",
        "\n",
        "print(\"PASSED: IG with different step counts\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cL20NLwthmR_"
      },
      "source": [
        "## Test: eap_integrated_gradients()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cElzjlAChmR_",
        "outputId": "50456ff7-4abd-4466-e09b-4cf2253a9078",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: torch.Size([1, 33])\n"
          ]
        }
      ],
      "source": [
        "# Get baseline embeddings for EAP-IG\n",
        "inputs = tokenizer([\"The capital of France is\"], thinking=False)\n",
        "seq_len = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "baseline_embeddings = ActivationDict(config, positions=slice(None))\n",
        "baseline_embeddings[(0, \"layer_in\")] = torch.zeros(\n",
        "    1, seq_len, config.hidden_size,\n",
        "    device=device,\n",
        "    dtype=model.dtype\n",
        ")\n",
        "\n",
        "print(f\"Input shape: {inputs['input_ids'].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hjE6Uzg2hmR_",
        "outputId": "b67f69f0-805a-4341-bf81-cc4748e4df56",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running EAP Integrated Gradients...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NNsightException",
          "evalue": "\n\nTraceback (most recent call last):\n  File \"/content/mech_interp_toolkit/src/mech_interp_toolkit/activations.py\", line 206, in unified_access_and_patching\n    comp[:] = self.patch_fn(comp, self.patching_dict[(layer, component)], patch_pos)\n  File \"/content/mech_interp_toolkit/src/mech_interp_toolkit/activations.py\", line 182, in patch_fn\n    original[:, position, :] = new_value\n\nRuntimeError: shape mismatch: value tensor of shape [33, 1024] cannot be broadcast to indexing result of shape [1, 1, 1024]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNNsightException\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-618734461.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running EAP Integrated Gradients...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m eap_ig = eap_integrated_gradients(\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/mech_interp_toolkit/src/mech_interp_toolkit/gradient_based_attribution.py\u001b[0m in \u001b[0;36meap_integrated_gradients\u001b[0;34m(model, inputs, baseline_embeddings, layer_components, metric_fn, position, intermediate_points)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mUAP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec_dict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0muap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m             \u001b[0macts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munified_access_and_patching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0macts\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Failed to retrieve activations.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/mech_interp_toolkit/src/mech_interp_toolkit/activations.py\u001b[0m in \u001b[0;36munified_access_and_patching\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    191\u001b[0m         with (\n\u001b[1;32m    192\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m         ):\n\u001b[1;32m    195\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloop_components\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nnsight/intervention/tracing/base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    663\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 665\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nnsight/intervention/backends/execution.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, tracer)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mwrap_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtracer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mGlobals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNNsightException\u001b[0m: \n\nTraceback (most recent call last):\n  File \"/content/mech_interp_toolkit/src/mech_interp_toolkit/activations.py\", line 206, in unified_access_and_patching\n    comp[:] = self.patch_fn(comp, self.patching_dict[(layer, component)], patch_pos)\n  File \"/content/mech_interp_toolkit/src/mech_interp_toolkit/activations.py\", line 182, in patch_fn\n    original[:, position, :] = new_value\n\nRuntimeError: shape mismatch: value tensor of shape [33, 1024] cannot be broadcast to indexing result of shape [1, 1, 1024]"
          ]
        }
      ],
      "source": [
        "# Run EAP-IG with specific layer components\n",
        "layer_components = [(0, \"attn\"), (0, \"mlp\"), (5, \"attn\"), (5, \"mlp\")]\n",
        "\n",
        "print(\"Running EAP Integrated Gradients...\")\n",
        "eap_ig = eap_integrated_gradients(\n",
        "    model=model,\n",
        "    inputs=inputs,\n",
        "    baseline_embeddings=baseline_embeddings,\n",
        "    layer_components=layer_components,\n",
        "    metric_fn=metric_fn,\n",
        "    position=-1,\n",
        "    intermediate_points=5,\n",
        ")\n",
        "\n",
        "print(f\"\\nEAP-IG computed for {len(eap_ig)} components\")\n",
        "for key, val in eap_ig.items():\n",
        "    print(f\"  {key}: {val}\")\n",
        "\n",
        "print(\"PASSED: EAP Integrated Gradients\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zT2DwLm_hmR_"
      },
      "outputs": [],
      "source": [
        "# Run EAP-IG with all layer components (default)\n",
        "print(\"\\nRunning EAP-IG with all layer components...\")\n",
        "eap_ig_full = eap_integrated_gradients(\n",
        "    model=model,\n",
        "    inputs=inputs,\n",
        "    baseline_embeddings=baseline_embeddings,\n",
        "    layer_components=None,  # Uses all components\n",
        "    metric_fn=metric_fn,\n",
        "    position=-1,\n",
        "    intermediate_points=3,\n",
        ")\n",
        "\n",
        "print(f\"EAP-IG (full): {len(eap_ig_full)} components\")\n",
        "\n",
        "# Print summary by layer\n",
        "print(\"\\nEAP-IG summary by layer:\")\n",
        "print(f\"{'Layer':<8} {'Attn':>12} {'MLP':>12}\")\n",
        "print(\"-\" * 32)\n",
        "\n",
        "for layer in range(min(10, config.num_hidden_layers)):\n",
        "    attn_val = eap_ig_full.get((layer, \"attn\"), torch.tensor(0.0))\n",
        "    mlp_val = eap_ig_full.get((layer, \"mlp\"), torch.tensor(0.0))\n",
        "    attn_score = attn_val.item() if isinstance(attn_val, torch.Tensor) else attn_val\n",
        "    mlp_score = mlp_val.item() if isinstance(mlp_val, torch.Tensor) else mlp_val\n",
        "    print(f\"{layer:<8} {attn_score:>12.6f} {mlp_score:>12.6f}\")\n",
        "\n",
        "print(\"PASSED: EAP-IG with all components\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avKZUyBghmR_"
      },
      "source": [
        "## Test: Custom metric functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAUccUfRhmR_"
      },
      "outputs": [],
      "source": [
        "# Test different metric functions\n",
        "def metric_sum(logits):\n",
        "    return logits.sum()\n",
        "\n",
        "def metric_mean(logits):\n",
        "    return logits.mean()\n",
        "\n",
        "def metric_max_prob(logits):\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    return probs.max()\n",
        "\n",
        "metrics = [\n",
        "    (\"sum\", metric_sum),\n",
        "    (\"mean\", metric_mean),\n",
        "    (\"max_prob\", metric_max_prob),\n",
        "]\n",
        "\n",
        "print(\"Testing EAP with different metrics:\")\n",
        "for name, metric in metrics:\n",
        "    eap = edge_attribution_patching(\n",
        "        model=model,\n",
        "        clean_inputs=clean_inputs,\n",
        "        corrupted_inputs=corrupted_inputs,\n",
        "        metric_fn=metric,\n",
        "        position=-1,\n",
        "    )\n",
        "    # Get total attribution\n",
        "    total = sum(v.sum().item() for v in eap.values())\n",
        "    print(f\"  {name}: total attribution = {total:.6f}\")\n",
        "\n",
        "print(\"PASSED: Custom metric functions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmVbWYU0hmSA"
      },
      "source": [
        "## Test: Different positions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2SwWhEwhmSA"
      },
      "outputs": [],
      "source": [
        "# Test EAP with different position specifications\n",
        "print(\"Testing EAP with different positions:\")\n",
        "\n",
        "# Last position\n",
        "eap_last = edge_attribution_patching(\n",
        "    model=model,\n",
        "    clean_inputs=clean_inputs,\n",
        "    corrupted_inputs=corrupted_inputs,\n",
        "    metric_fn=metric_fn,\n",
        "    position=-1,\n",
        ")\n",
        "print(f\"  position=-1: {len(eap_last)} components\")\n",
        "\n",
        "# All positions\n",
        "eap_all = edge_attribution_patching(\n",
        "    model=model,\n",
        "    clean_inputs=clean_inputs,\n",
        "    corrupted_inputs=corrupted_inputs,\n",
        "    metric_fn=metric_fn,\n",
        "    position=slice(None),\n",
        ")\n",
        "print(f\"  position=slice(None): {len(eap_all)} components\")\n",
        "\n",
        "print(\"PASSED: Different positions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCy0yvtohmSA"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8sPDmINhmSA"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*50)\n",
        "print(\"All gradient_based_attribution module tests PASSED!\")\n",
        "print(\"=\"*50)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}