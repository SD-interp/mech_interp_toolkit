{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !git clone https://github.com/SD-interp/mech_interp_toolkit.git\n",
        "# %cd mech_interp_toolkit\n",
        "# !pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test: gradient_based_attribution module\n",
        "\n",
        "This notebook tests gradient-based attribution methods from `mech_interp_toolkit.gradient_based_attribution`.\n",
        "\n",
        "Functions tested:\n",
        "- `get_activations()` - Extract activations with gradient support\n",
        "- `get_embeddings()` - Extract input embeddings\n",
        "- `simple_integrated_gradients()` - Vanilla integrated gradients w.r.t embeddings\n",
        "- `edge_attribution_patching()` - Simple gradient x activation method\n",
        "- `eap_integrated_gradients()` - Integrated gradients for edge attributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(os.path.abspath(\"../src\"))\n",
        "\n",
        "import torch\n",
        "from mech_interp_toolkit.utils import load_model_tokenizer_config, get_default_device, get_all_layer_components\n",
        "from mech_interp_toolkit.gradient_based_attribution import (\n",
        "    get_activations,\n",
        "    get_embeddings,\n",
        "    simple_integrated_gradients,\n",
        "    edge_attribution_patching,\n",
        "    eap_integrated_gradients,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup: Load model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = \"Qwen/Qwen3-0.6B\"\n",
        "device = get_default_device()\n",
        "\n",
        "print(f\"Loading model {model_name} on {device}...\")\n",
        "model, tokenizer, config = load_model_tokenizer_config(model_name, device=device)\n",
        "print(\"Model loaded successfully\")\n",
        "print(f\"Number of layers: {config.num_hidden_layers}\")\n",
        "print(f\"Hidden size: {config.hidden_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare clean and corrupted inputs for attribution methods\n",
        "clean_prompts = [\"The capital of France is\"]\n",
        "corrupted_prompts = [\"The capital of Germany is\"]\n",
        "\n",
        "clean_inputs = tokenizer(clean_prompts, thinking=False)\n",
        "corrupted_inputs = tokenizer(corrupted_prompts, thinking=False)\n",
        "\n",
        "print(f\"Clean input shape: {clean_inputs['input_ids'].shape}\")\n",
        "print(f\"Corrupted input shape: {corrupted_inputs['input_ids'].shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test: get_activations()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test extracting activations from specific layer components\n",
        "layer_components = [(0, \"attn\"), (0, \"mlp\"), (5, \"attn\"), (5, \"mlp\")]\n",
        "\n",
        "print(\"Extracting activations...\")\n",
        "activations = get_activations(model, clean_inputs, layer_components)\n",
        "\n",
        "print(f\"\\nExtracted activations:\")\n",
        "for key, val in activations.items():\n",
        "    print(f\"  {key}: {val.shape}\")\n",
        "\n",
        "# Verify all requested components were extracted\n",
        "for lc in layer_components:\n",
        "    assert lc in activations, f\"Missing {lc}\"\n",
        "print(\"\\nPASSED: get_activations()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test: get_embeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test extracting input embeddings\n",
        "print(\"Extracting embeddings...\")\n",
        "embeddings = get_embeddings(model, clean_inputs)\n",
        "\n",
        "print(f\"Embeddings keys: {list(embeddings.keys())}\")\n",
        "print(f\"Embeddings shape: {embeddings[(0, 'layer_in')].shape}\")\n",
        "\n",
        "# Should have shape (batch, seq_len, hidden_size)\n",
        "batch_size, seq_len = clean_inputs[\"input_ids\"].shape\n",
        "assert embeddings[(0, \"layer_in\")].shape == (batch_size, seq_len, config.hidden_size)\n",
        "print(\"\\nPASSED: get_embeddings()\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test: edge_attribution_patching()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define a simple metric function\n",
        "def metric_fn(logits):\n",
        "    return logits[:, -1, :].max(dim=-1).values.sum()\n",
        "\n",
        "print(\"Running Edge Attribution Patching (EAP)...\")\n",
        "eap_scores = edge_attribution_patching(\n",
        "    model=model,\n",
        "    clean_inputs=clean_inputs,\n",
        "    corrupted_inputs=corrupted_inputs,\n",
        "    compute_grad_at=\"clean\",\n",
        "    metric_fn=metric_fn,\n",
        ")\n",
        "\n",
        "print(f\"\\nEAP scores computed for {len(eap_scores)} components\")\n",
        "print(f\"EAP score type: {type(eap_scores).__name__}\")\n",
        "print(f\"\\nSample EAP scores (first 6 components):\")\n",
        "for key, score in list(eap_scores.items())[:6]:\n",
        "    print(f\"  {key}: {score}\")\n",
        "\n",
        "print(\"\\nPASSED: edge_attribution_patching()\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with compute_grad_at=\"corrupted\"\n",
        "print(\"Running EAP with compute_grad_at='corrupted'...\")\n",
        "eap_scores_corrupted = edge_attribution_patching(\n",
        "    model=model,\n",
        "    clean_inputs=clean_inputs,\n",
        "    corrupted_inputs=corrupted_inputs,\n",
        "    compute_grad_at=\"corrupted\",\n",
        "    metric_fn=metric_fn,\n",
        ")\n",
        "\n",
        "print(f\"EAP scores (corrupted): {len(eap_scores_corrupted)} components\")\n",
        "print(\"Sample scores:\")\n",
        "for key in list(eap_scores_corrupted.keys())[:3]:\n",
        "    print(f\"  {key}: {eap_scores_corrupted[key]}\")\n",
        "\n",
        "print(\"\\nPASSED: EAP with corrupted gradients\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze EAP scores by layer\n",
        "n_layers = config.num_hidden_layers\n",
        "\n",
        "print(\"\\nEAP scores summary by layer (first 10 layers):\")\n",
        "print(f\"{'Layer':<8} {'Attn':>12} {'MLP':>12}\")\n",
        "print(\"-\" * 32)\n",
        "\n",
        "for layer in range(min(10, n_layers)):\n",
        "    attn_key = (layer, \"attn\")\n",
        "    mlp_key = (layer, \"mlp\")\n",
        "\n",
        "    attn_score = eap_scores.get(attn_key, torch.tensor(0.0)).item() if attn_key in eap_scores else 0.0\n",
        "    mlp_score = eap_scores.get(mlp_key, torch.tensor(0.0)).item() if mlp_key in eap_scores else 0.0\n",
        "\n",
        "    print(f\"{layer:<8} {attn_score:>12.6f} {mlp_score:>12.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test: simple_integrated_gradients()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get input embeddings to determine shape for baseline\n",
        "inputs = tokenizer([\"The quick brown fox\"], thinking=False)\n",
        "embeddings = get_embeddings(model, inputs)\n",
        "input_embeddings = embeddings[(0, \"layer_in\")]\n",
        "\n",
        "# Create baseline embeddings (zeros) - must be a torch.Tensor, same shape as input embeddings\n",
        "baseline_embeddings = torch.zeros_like(input_embeddings)\n",
        "\n",
        "print(f\"Input sequence length: {inputs['input_ids'].shape[1]}\")\n",
        "print(f\"Input embeddings shape: {input_embeddings.shape}\")\n",
        "print(f\"Baseline embeddings shape: {baseline_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Running Simple Integrated Gradients...\")\n",
        "with torch.enable_grad():\n",
        "    ig_attributions = simple_integrated_gradients(\n",
        "        model=model,\n",
        "        inputs=inputs,\n",
        "        baseline_embeddings=baseline_embeddings,\n",
        "        metric_fn=metric_fn,\n",
        "        steps=10,  # Using fewer steps for faster testing\n",
        "    )\n",
        "\n",
        "print(f\"\\nIG attributions type: {type(ig_attributions).__name__}\")\n",
        "print(f\"IG attributions shape: {ig_attributions.shape}\")\n",
        "\n",
        "# Should return tensor of shape (batch, seq_len) - attributions summed over hidden dim\n",
        "assert ig_attributions.ndim == 2, \"IG should return 2D tensor (batch, seq_len)\"\n",
        "print(\"\\nPASSED: simple_integrated_gradients()\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze IG attributions per position\n",
        "print(f\"\\nIG attribution per position:\")\n",
        "for pos in range(ig_attributions.shape[1]):\n",
        "    val = ig_attributions[0, pos].item()\n",
        "    print(f\"  Position {pos}: {val:.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test: eap_integrated_gradients()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare inputs for EAP-IG (requires clean and corrupted inputs with same shape)\n",
        "print(f\"Clean inputs shape: {clean_inputs['input_ids'].shape}\")\n",
        "print(f\"Corrupted inputs shape: {corrupted_inputs['input_ids'].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Running EAP Integrated Gradients...\")\n",
        "with torch.enable_grad():\n",
        "    eap_ig = eap_integrated_gradients(\n",
        "        model=model,\n",
        "        clean_dict=clean_inputs,\n",
        "        corrupted_dict=corrupted_inputs,\n",
        "        metric_fn=metric_fn,\n",
        "        intermediate_points=5,\n",
        "    )\n",
        "\n",
        "print(f\"\\nEAP-IG computed for {len(eap_ig)} components\")\n",
        "print(f\"EAP-IG type: {type(eap_ig).__name__}\")\n",
        "\n",
        "print(\"\\nSample EAP-IG scores (first 6 components):\")\n",
        "for key, val in list(eap_ig.items())[:6]:\n",
        "    print(f\"  {key}: {val}\")\n",
        "\n",
        "print(\"\\nPASSED: eap_integrated_gradients()\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze EAP-IG scores by layer\n",
        "print(\"\\nEAP-IG summary by layer (first 10 layers):\")\n",
        "print(f\"{'Layer':<8} {'Attn':>12} {'MLP':>12}\")\n",
        "print(\"-\" * 32)\n",
        "\n",
        "for layer in range(min(10, n_layers)):\n",
        "    attn_val = eap_ig.get((layer, \"attn\"), torch.tensor(0.0))\n",
        "    mlp_val = eap_ig.get((layer, \"mlp\"), torch.tensor(0.0))\n",
        "    attn_score = attn_val.item() if isinstance(attn_val, torch.Tensor) else attn_val\n",
        "    mlp_score = mlp_val.item() if isinstance(mlp_val, torch.Tensor) else mlp_val\n",
        "    print(f\"{layer:<8} {attn_score:>12.6f} {mlp_score:>12.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test: Custom metric functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test different metric functions with edge_attribution_patching\n",
        "def metric_sum(logits):\n",
        "    return logits.sum()\n",
        "\n",
        "def metric_mean(logits):\n",
        "    return logits.mean()\n",
        "\n",
        "def metric_max_prob(logits):\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    return probs.max()\n",
        "\n",
        "metrics = [\n",
        "    (\"sum\", metric_sum),\n",
        "    (\"mean\", metric_mean),\n",
        "    (\"max_prob\", metric_max_prob),\n",
        "]\n",
        "\n",
        "print(\"Testing EAP with different metrics:\")\n",
        "for name, metric in metrics:\n",
        "    eap = edge_attribution_patching(\n",
        "        model=model,\n",
        "        clean_inputs=clean_inputs,\n",
        "        corrupted_inputs=corrupted_inputs,\n",
        "        metric_fn=metric,\n",
        "    )\n",
        "    # Get total attribution\n",
        "    total = sum(v.sum().item() for v in eap.values())\n",
        "    print(f\"  {name}: total attribution = {total:.6f}\")\n",
        "\n",
        "print(\"\\nPASSED: Custom metric functions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test: Error handling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test that gradient-based methods require gradients enabled\n",
        "print(\"Testing error handling for disabled gradients...\")\n",
        "\n",
        "try:\n",
        "    with torch.no_grad():\n",
        "        _ = simple_integrated_gradients(\n",
        "            model=model,\n",
        "            inputs=inputs,\n",
        "            baseline_embeddings=baseline_embeddings,\n",
        "            metric_fn=metric_fn,\n",
        "            steps=5,\n",
        "        )\n",
        "    print(\"ERROR: Should have raised RuntimeError\")\n",
        "except RuntimeError as e:\n",
        "    print(f\"Correctly raised RuntimeError: {e}\")\n",
        "\n",
        "print(\"\\nPASSED: Error handling\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test shape mismatch validation for simple_integrated_gradients\n",
        "print(\"Testing shape mismatch validation...\")\n",
        "\n",
        "# Create baseline with wrong shape\n",
        "wrong_baseline = torch.zeros(1, 5, config.hidden_size, device=device, dtype=model.dtype)\n",
        "\n",
        "try:\n",
        "    with torch.enable_grad():\n",
        "        _ = simple_integrated_gradients(\n",
        "            model=model,\n",
        "            inputs=inputs,\n",
        "            baseline_embeddings=wrong_baseline,\n",
        "            metric_fn=metric_fn,\n",
        "            steps=5,\n",
        "        )\n",
        "    print(\"ERROR: Should have raised ValueError\")\n",
        "except ValueError as e:\n",
        "    print(f\"Correctly raised ValueError: {e}\")\n",
        "\n",
        "print(\"\\nPASSED: Shape mismatch validation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*50)\n",
        "print(\"All gradient_based_attribution module tests PASSED!\")\n",
        "print(\"=\"*50)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
