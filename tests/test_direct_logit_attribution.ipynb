{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test: direct_logit_attribution module\n",
    "\n",
    "This notebook tests Direct Logit Attribution (DLA) functions from `mech_interp_toolkit.direct_logit_attribution`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "import torch\n",
    "from mech_interp_toolkit.utils import load_model_tokenizer_config, get_default_device\n",
    "from mech_interp_toolkit.direct_logit_attribution import (\n",
    "    get_pre_rms_logit_diff_direction,\n",
    "    run_componentwise_dla,\n",
    "    run_headwise_dla_for_layer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "device = get_default_device()\n",
    "\n",
    "print(f\"Loading model {model_name} on {device}...\")\n",
    "model, tokenizer, config = load_model_tokenizer_config(model_name, device=device)\n",
    "print(\"Model loaded successfully\")\n",
    "print(f\"Number of layers: {config.num_hidden_layers}\")\n",
    "print(f\"Hidden size: {config.hidden_size}\")\n",
    "print(f\"Number of attention heads: {config.num_attention_heads}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: get_pre_rms_logit_diff_direction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with valid single-token pairs\n",
    "token_pair = [\"A\", \"B\"]\n",
    "\n",
    "print(f\"Computing logit diff direction for tokens: {token_pair}\")\n",
    "direction = get_pre_rms_logit_diff_direction(token_pair, tokenizer, model)\n",
    "\n",
    "print(f\"Direction shape: {direction.shape}\")\n",
    "print(f\"Direction norm: {direction.norm().item():.4f}\")\n",
    "print(f\"Direction device: {direction.device}\")\n",
    "\n",
    "assert direction.shape == (config.hidden_size,), f\"Expected shape ({config.hidden_size},), got {direction.shape}\"\n",
    "assert direction.norm().item() > 0, \"Direction should not be zero\"\n",
    "print(\"PASSED: Basic logit diff direction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different token pairs\n",
    "token_pairs_to_test = [\n",
    "    [\"Yes\", \"No\"],\n",
    "    [\"true\", \"false\"],\n",
    "    [\"1\", \"0\"],\n",
    "]\n",
    "\n",
    "for pair in token_pairs_to_test:\n",
    "    try:\n",
    "        direction = get_pre_rms_logit_diff_direction(pair, tokenizer, model)\n",
    "        print(f\"Tokens {pair}: direction norm = {direction.norm().item():.4f}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Tokens {pair}: Skipped - {e}\")\n",
    "\n",
    "print(\"PASSED: Multiple token pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test error handling: wrong number of tokens\n",
    "try:\n",
    "    direction = get_pre_rms_logit_diff_direction([\"A\"], tokenizer, model)\n",
    "    assert False, \"Should have raised ValueError\"\n",
    "except ValueError as e:\n",
    "    print(f\"Correctly raised error for single token: {e}\")\n",
    "\n",
    "try:\n",
    "    direction = get_pre_rms_logit_diff_direction([\"A\", \"B\", \"C\"], tokenizer, model)\n",
    "    assert False, \"Should have raised ValueError\"\n",
    "except ValueError as e:\n",
    "    print(f\"Correctly raised error for three tokens: {e}\")\n",
    "\n",
    "print(\"PASSED: Error handling for wrong token count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: run_componentwise_dla()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare inputs\n",
    "prompts = [\n",
    "    \"The answer is definitely\",\n",
    "    \"I think the result is\"\n",
    "]\n",
    "inputs = tokenizer(prompts, thinking=False)\n",
    "print(f\"Input shape: {inputs['input_ids'].shape}\")\n",
    "\n",
    "# Get direction\n",
    "direction = get_pre_rms_logit_diff_direction([\"A\", \"B\"], tokenizer, model)\n",
    "direction = direction.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running component-wise DLA...\")\n",
    "dla_scores = run_componentwise_dla(model, inputs, direction)\n",
    "\n",
    "print(f\"\\nDLA scores computed for {len(dla_scores)} components:\")\n",
    "for key, score in list(dla_scores.items())[:10]:  # Show first 10\n",
    "    print(f\"  {key}: {score}\")\n",
    "if len(dla_scores) > 10:\n",
    "    print(f\"  ... and {len(dla_scores) - 10} more\")\n",
    "\n",
    "# Check that we have scores for attn and mlp at each layer\n",
    "n_layers = config.num_hidden_layers\n",
    "expected_keys = [(i, \"attn\") for i in range(n_layers)] + [(i, \"mlp\") for i in range(n_layers)]\n",
    "\n",
    "for key in expected_keys:\n",
    "    assert key in dla_scores, f\"Missing DLA score for {key}\"\n",
    "\n",
    "print(f\"\\nTotal components with DLA scores: {len(dla_scores)}\")\n",
    "print(\"PASSED: Component-wise DLA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze DLA scores\n",
    "attn_scores = [dla_scores[(i, \"attn\")].mean().item() for i in range(n_layers)]\n",
    "mlp_scores = [dla_scores[(i, \"mlp\")].mean().item() for i in range(n_layers)]\n",
    "\n",
    "print(\"Mean DLA scores by layer:\")\n",
    "print(f\"{'Layer':<8} {'Attn':>12} {'MLP':>12}\")\n",
    "print(\"-\" * 32)\n",
    "for i in range(n_layers):\n",
    "    print(f\"{i:<8} {attn_scores[i]:>12.4f} {mlp_scores[i]:>12.4f}\")\n",
    "\n",
    "print(f\"\\nTotal Attn contribution: {sum(attn_scores):.4f}\")\n",
    "print(f\"Total MLP contribution: {sum(mlp_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: run_headwise_dla_for_layer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for a specific layer\n",
    "layer = 10\n",
    "\n",
    "print(f\"Running head-wise DLA for layer {layer}...\")\n",
    "head_dla = run_headwise_dla_for_layer(model, inputs, direction, layer)\n",
    "\n",
    "print(f\"Head DLA shape: {head_dla.shape}\")\n",
    "print(f\"Expected: (batch_size, num_heads) = ({inputs['input_ids'].shape[0]}, {config.num_attention_heads})\")\n",
    "\n",
    "assert head_dla.shape == (inputs['input_ids'].shape[0], config.num_attention_heads), \\\n",
    "    f\"Shape mismatch: got {head_dla.shape}\"\n",
    "\n",
    "print(f\"\\nHead DLA scores for layer {layer}:\")\n",
    "for head_idx in range(config.num_attention_heads):\n",
    "    mean_score = head_dla[:, head_idx].mean().item()\n",
    "    print(f\"  Head {head_idx}: {mean_score:.6f}\")\n",
    "\n",
    "print(\"PASSED: Head-wise DLA for layer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test head-wise DLA for multiple layers\n",
    "print(\"\\nHead-wise DLA summary across layers:\")\n",
    "print(f\"{'Layer':<8} {'Max Head':>10} {'Max Score':>12} {'Min Score':>12}\")\n",
    "print(\"-\" * 44)\n",
    "\n",
    "for layer in [0, 5, 10, 15, n_layers - 1]:\n",
    "    if layer >= n_layers:\n",
    "        continue\n",
    "    head_dla = run_headwise_dla_for_layer(model, inputs, direction, layer)\n",
    "    mean_scores = head_dla.mean(dim=0)  # Average across batch\n",
    "    max_head = mean_scores.argmax().item()\n",
    "    max_score = mean_scores.max().item()\n",
    "    min_score = mean_scores.min().item()\n",
    "    print(f\"{layer:<8} {max_head:>10} {max_score:>12.6f} {min_score:>12.6f}\")\n",
    "\n",
    "print(\"PASSED: Head-wise DLA across layers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: DLA with different inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with single input\n",
    "single_input = tokenizer([\"What is 2 + 2?\"], thinking=False)\n",
    "\n",
    "dla_scores_single = run_componentwise_dla(model, single_input, direction)\n",
    "print(f\"Single input DLA: {len(dla_scores_single)} components\")\n",
    "\n",
    "# Check score shapes for single batch\n",
    "sample_score = dla_scores_single[(0, \"attn\")]\n",
    "print(f\"Sample score shape: {sample_score.shape}\")\n",
    "print(\"PASSED: DLA with single input\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with custom direction (random)\n",
    "random_direction = torch.randn(config.hidden_size, device=device)\n",
    "random_direction = random_direction / random_direction.norm()  # Normalize\n",
    "\n",
    "dla_scores_random = run_componentwise_dla(model, inputs, random_direction)\n",
    "\n",
    "print(f\"DLA with random direction: {len(dla_scores_random)} components\")\n",
    "print(f\"Sample attn score (layer 0): {dla_scores_random[(0, 'attn')]}\")\n",
    "print(\"PASSED: DLA with custom direction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Consistency check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run DLA twice with same inputs - should get same results\n",
    "dla_scores_1 = run_componentwise_dla(model, inputs, direction)\n",
    "dla_scores_2 = run_componentwise_dla(model, inputs, direction)\n",
    "\n",
    "all_close = True\n",
    "for key in dla_scores_1.keys():\n",
    "    if not torch.allclose(dla_scores_1[key], dla_scores_2[key], rtol=1e-4):\n",
    "        all_close = False\n",
    "        print(f\"Mismatch at {key}\")\n",
    "        break\n",
    "\n",
    "assert all_close, \"DLA results should be consistent\"\n",
    "print(\"PASSED: DLA consistency check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"All direct_logit_attribution module tests PASSED!\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
