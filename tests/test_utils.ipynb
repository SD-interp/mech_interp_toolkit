{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test: utils module\n",
    "\n",
    "This notebook tests all utility functions in `mech_interp_toolkit.utils`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from mech_interp_toolkit.utils import (\n",
    "    set_global_seed,\n",
    "    load_model_tokenizer_config,\n",
    "    get_position_ids,\n",
    "    input_dict_to_tuple,\n",
    "    get_logit_difference,\n",
    "    regularize_position,\n",
    "    get_num_layers,\n",
    "    get_all_layer_components,\n",
    "    get_default_device,\n",
    "    build_dataloader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: get_default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = get_default_device()\n",
    "print(f\"Default device: {device}\")\n",
    "assert device in [\"cuda\", \"cpu\"], \"Device must be 'cuda' or 'cpu'\"\n",
    "print(\"PASSED: get_default_device()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: set_global_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reproducibility\n",
    "set_global_seed(42)\n",
    "r1 = random.random()\n",
    "n1 = np.random.rand()\n",
    "t1 = torch.rand(1).item()\n",
    "\n",
    "set_global_seed(42)\n",
    "r2 = random.random()\n",
    "n2 = np.random.rand()\n",
    "t2 = torch.rand(1).item()\n",
    "\n",
    "assert r1 == r2, \"Random seed not reproducible\"\n",
    "assert n1 == n2, \"Numpy seed not reproducible\"\n",
    "assert t1 == t2, \"Torch seed not reproducible\"\n",
    "print(f\"Random: {r1} == {r2}\")\n",
    "print(f\"Numpy: {n1} == {n2}\")\n",
    "print(f\"Torch: {t1} == {t2}\")\n",
    "print(\"PASSED: set_global_seed()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: load_model_tokenizer_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen3-0.6B\"\n",
    "device = get_default_device()\n",
    "\n",
    "print(f\"Loading model {model_name} on {device}...\")\n",
    "model, tokenizer, config = load_model_tokenizer_config(model_name, device=device)\n",
    "print(f\"Model loaded successfully\")\n",
    "print(f\"Config type: {type(config).__name__}\")\n",
    "print(f\"Number of layers: {config.num_hidden_layers}\")\n",
    "print(f\"Hidden size: {config.hidden_size}\")\n",
    "print(f\"Number of attention heads: {config.num_attention_heads}\")\n",
    "print(\"PASSED: load_model_tokenizer_config()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: get_position_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with padded attention mask (left padding)\n",
    "attention_mask = torch.tensor([\n",
    "    [0, 0, 1, 1, 1],  # 2 padding tokens\n",
    "    [0, 1, 1, 1, 1],  # 1 padding token\n",
    "    [1, 1, 1, 1, 1],  # no padding\n",
    "])\n",
    "\n",
    "position_ids = get_position_ids(attention_mask)\n",
    "print(f\"Attention mask:\\n{attention_mask}\")\n",
    "print(f\"Position IDs:\\n{position_ids}\")\n",
    "\n",
    "# Expected: position_ids for non-padded tokens should be 0, 1, 2, ...\n",
    "expected = torch.tensor([\n",
    "    [1, 1, 0, 1, 2],\n",
    "    [1, 0, 1, 2, 3],\n",
    "    [0, 1, 2, 3, 4],\n",
    "])\n",
    "assert torch.equal(position_ids, expected), f\"Position IDs mismatch. Got:\\n{position_ids}\\nExpected:\\n{expected}\"\n",
    "print(\"PASSED: get_position_ids()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: input_dict_to_tuple()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample input dict\n",
    "input_dict = {\n",
    "    \"input_ids\": torch.tensor([[1, 2, 3, 4]]),\n",
    "    \"attention_mask\": torch.tensor([[1, 1, 1, 1]]),\n",
    "}\n",
    "\n",
    "input_ids, attention_mask, position_ids = input_dict_to_tuple(input_dict, device=\"cpu\")\n",
    "\n",
    "print(f\"Input IDs shape: {input_ids.shape}\")\n",
    "print(f\"Attention mask shape: {attention_mask.shape}\")\n",
    "print(f\"Position IDs shape: {position_ids.shape}\")\n",
    "print(f\"Position IDs: {position_ids}\")\n",
    "\n",
    "assert input_ids.shape == (1, 4), \"Input IDs shape mismatch\"\n",
    "assert attention_mask.shape == (1, 4), \"Attention mask shape mismatch\"\n",
    "assert position_ids.shape == (1, 4), \"Position IDs shape mismatch\"\n",
    "print(\"PASSED: input_dict_to_tuple()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: regularize_position()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test int input\n",
    "result = regularize_position(5)\n",
    "assert result == [5], f\"Expected [5], got {result}\"\n",
    "print(f\"regularize_position(5) = {result}\")\n",
    "\n",
    "# Test None input\n",
    "result = regularize_position(None)\n",
    "assert result == slice(None), f\"Expected slice(None), got {result}\"\n",
    "print(f\"regularize_position(None) = {result}\")\n",
    "\n",
    "# Test slice input\n",
    "result = regularize_position(slice(1, 5))\n",
    "assert result == slice(1, 5), f\"Expected slice(1, 5), got {result}\"\n",
    "print(f\"regularize_position(slice(1, 5)) = {result}\")\n",
    "\n",
    "# Test list input\n",
    "result = regularize_position([1, 2, 3])\n",
    "assert result == [1, 2, 3], f\"Expected [1, 2, 3], got {result}\"\n",
    "print(f\"regularize_position([1, 2, 3]) = {result}\")\n",
    "\n",
    "print(\"PASSED: regularize_position()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: get_num_layers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = get_num_layers(model)\n",
    "print(f\"Number of layers: {n_layers}\")\n",
    "assert n_layers == config.num_hidden_layers, \"Layer count mismatch\"\n",
    "assert isinstance(n_layers, int), \"Layer count should be an integer\"\n",
    "print(\"PASSED: get_num_layers()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: get_all_layer_components()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_components = get_all_layer_components(model)\n",
    "print(f\"Number of layer components: {len(layer_components)}\")\n",
    "print(f\"First 6 components: {layer_components[:6]}\")\n",
    "\n",
    "# Should have 2 components (attn, mlp) per layer\n",
    "expected_count = n_layers * 2\n",
    "assert len(layer_components) == expected_count, f\"Expected {expected_count} components, got {len(layer_components)}\"\n",
    "\n",
    "# Check ordering: (0, attn), (0, mlp), (1, attn), (1, mlp), ...\n",
    "assert layer_components[0] == (0, \"attn\"), \"First component should be (0, 'attn')\"\n",
    "assert layer_components[1] == (0, \"mlp\"), \"Second component should be (0, 'mlp')\"\n",
    "print(\"PASSED: get_all_layer_components()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: get_logit_difference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get some logits from the model\n",
    "prompts = [\"The answer is\"]\n",
    "inputs = tokenizer(prompts, thinking=False)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.model(inputs[\"input_ids\"].to(device), attention_mask=inputs[\"attention_mask\"].to(device))\n",
    "    logits = outputs.logits[:, -1, :]  # Last position logits\n",
    "\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "\n",
    "# Test logit difference between two tokens\n",
    "tokens = [\"A\", \"B\"]\n",
    "logit_diff = get_logit_difference(logits, tokenizer, tokens)\n",
    "print(f\"Logit difference (A - B): {logit_diff.item():.4f}\")\n",
    "\n",
    "assert logit_diff.shape == (1,), f\"Expected shape (1,), got {logit_diff.shape}\"\n",
    "print(\"PASSED: get_logit_difference()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: build_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with tensor dataset\n",
    "tensor_data = torch.randn(100, 10)\n",
    "dataloader = build_dataloader(tensor_data, batch_size=16)\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "print(f\"Batch shape: {batch.shape}\")\n",
    "assert batch.shape == (16, 10), f\"Expected shape (16, 10), got {batch.shape}\"\n",
    "\n",
    "# Test with list dataset\n",
    "list_data = list(range(50))\n",
    "dataloader = build_dataloader(list_data, batch_size=10)\n",
    "\n",
    "batch = next(iter(dataloader))\n",
    "print(f\"List batch: {batch}\")\n",
    "assert len(batch) == 10, f\"Expected batch size 10, got {len(batch)}\"\n",
    "\n",
    "print(\"PASSED: build_dataloader()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"All utils module tests PASSED!\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
